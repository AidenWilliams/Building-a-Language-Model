{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Imports](#Imports)\n",
    "2.  [Introduction](#Introduction)\n",
    "3.  [Documentation](#Documentation)\n",
    "4.  [Corpus](#Corpus)\n",
    "5.  [Model](#Model)\n",
    "6.  [Evaluation](#Evaluation)\n",
    "7.  [References](#References)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "use command  pip install -r requirements.txt\n",
    "\n",
    "8 libraries are needed for this project:\n",
    "* numpy - Word filtering\n",
    "* lxml - To read broken xml files\n",
    "* re - To split lines with regex\n",
    "* tqdm.notebook - tqdm progress bars, but for ipynb files\n",
    "* os - File traversal\n",
    "* collections - for defaultdict\n",
    "* LanguageModel - Contains the Corpus and Model class (same code), split for cleanliness\n",
    "* random - For sampling\n",
    "* sklearn - Splitting train and test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Model' from 'LanguageModel' (C:\\Users\\Aiden Williams\\PycharmProjects\\NLP\\LanguageModel\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-a039de4113c1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0moperator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mLanguageModel\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mCorpus\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mLanguageModel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'Model' from 'LanguageModel' (C:\\Users\\Aiden Williams\\PycharmProjects\\NLP\\LanguageModel\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import operator\n",
    "from LanguageModel import Corpus\n",
    "import LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook I document the code I wrote in LanguageModel.py and then I use the Corpus and Model objects to evaluate\n",
    "and create a number of language models based off the Korpus Malti dataset [[1]](#References).\n",
    "\n",
    "# Documentation\n",
    "\n",
    "In this section I will go over the code in LanguageModel.py and my design choices."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Corpus\n",
    "\n",
    "*keywords*: Corpus, NGram, Model\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The corpus object mainly represents the pre-processed data of a given *corpus* as well as its processed form as NGrams\n",
    "and probability models. It can also be seen as a paragraph of sentences.\n",
    "\n",
    "On initialization the *corpus* data is read and stored as a list of sentences, where each sentence is a list of words,\n",
    "where each word is a string.\n",
    "\n",
    ">Previously I had a Word object that would retain the **4 columns**, however this was causing a lot of clutter and would\n",
    "> interfere with the sentence generation process, so I dropped it for the simpler string.\n",
    "\n",
    "A vanilla unigram and model are also created at this time.\n",
    "\n",
    "The option is given to the user to create a corpus object from an existing list of lists of strings (another corpus).\n",
    "\n",
    "### Usage\n",
    "\n",
    "The corpus object is intended to be used as a non static object for one set of xml files. This would give a user the\n",
    "ability to have multiple corpus objects that load in different xml files.\n",
    "\n",
    "Once created the ```Model()```,  ```NGram()```, ```LinearInterpolation()``` and ```GenerateSentence()``` functions can\n",
    "be used to efficiently give the desired output. As for traversing the corpus list itself, this can be done by accessing\n",
    "```self``` as ```__len___()```, ```__iter()__``` and ```__getitem()__``` are written for this purpose.\n",
    "\n",
    "*Perplexity* is handled by the individual Model.\n",
    "\n",
    "All 3 attributes; ```_corpus```,```_ngrams```,```_models``` are intended to be private."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Explanation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _ReadCorpus\n",
    "\n",
    "This function checks the root location of where all xml files are contained and if it encounters no issue accessing it,\n",
    "it will read the contents of the files within it and return them in the form of a list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _ReadCorpus(root='Corpus/', verbose=False):\n",
    "    if not os.access(root, os.R_OK):\n",
    "        print('Check root!!')\n",
    "\n",
    "    xml_data = []\n",
    "\n",
    "    for file in tqdm(os.listdir(root), desc='Reading Files', disable=not verbose):\n",
    "        xml_data.append(open(os.path.join(root, file), 'r', encoding='utf8').read())  # Read file\n",
    "    return xml_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _ParseAsXML\n",
    "\n",
    "The parser being used is initialised and the xml data from the files is read into ```xml_data```. Each file is then parsed\n",
    "and appended to ```roots```. Each file is split in a number of texts, so ```roots``` is a list of these parsed texts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _ParseAsXML(root='Corpus/', verbose=False):\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    roots = []\n",
    "    xml_data = Corpus._ReadCorpus(root, verbose)\n",
    "    for xml in tqdm(xml_data, desc='Parsing XML', disable=not verbose):\n",
    "        roots.append(etree.fromstring(xml, parser=parser))\n",
    "    return roots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### filterFurther\n",
    "\n",
    "Filters a given word from spaces, symbols and removes capitalization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filterFurther(word: str):\n",
    "    # remove symbols\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n,\"\n",
    "    for i in range(len(symbols)):\n",
    "        word = np.char.replace(word, symbols[i], ' ')\n",
    "    # remove apostrophe\n",
    "    word = np.char.replace(word, \"'\", \"\")\n",
    "    # Remove any double spaces\n",
    "    word = np.char.replace(word, \" \", \"\")\n",
    "\n",
    "    word = str(np.char.lower(word))\n",
    "    if word == \"\" or word == \" \":\n",
    "        return None\n",
    "    return word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CorpusAsListOfSentences\n",
    "\n",
    "This function is the last step in creating a pre-processed corpus.\n",
    "\n",
    "```sentences``` a python list is initialised.\n",
    "\n",
    "It is important to understand the Maltese dataset xml structure to properly understand this step.\n",
    "\n",
    "Each text has a number of paragraphs, and each paragraph has a number of sentences, each of these containing a number of\n",
    "words. Every word has 4 values; word, speech tag, lemma and morphological root.\n",
    "\n",
    "The 3 nested loops show the movement from sentence to sentence. Each sentence is filtered using regex. This filter splits\n",
    "the sentence into a list of words (including the extra values). Now each word has its extra values removed. Each word\n",
    "is filtered to make the model creation simpler. The start and end tags are added in the appropriate place.\n",
    "\n",
    "\n",
    "#### Previous Version\n",
    "Before settling on this I attempted to have the *corpus* as a Pandas dataframe. This would have been useful if I\n",
    "continued on developing a tensor oriented approach to the problem. However, since I scrapped that idea since a pd dataframe was\n",
    "causing too much clutter as all the sentences would have to be the same length."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CorpusAsListOfSentences(root='Corpus/', verbose=False):\n",
    "    roots = Corpus._ParseAsXML(root, verbose)\n",
    "    sentences = []\n",
    "    for root in tqdm(roots, desc='Building Sentences', disable=not verbose):\n",
    "        for i, p in tqdm(enumerate(root), desc='Paragraph', disable=not verbose):\n",
    "            for k, s in enumerate(p):\n",
    "                unfiltered_sentence = re.split(r'\\n', s.text.lstrip('\\n'))\n",
    "                sentence = []\n",
    "                for unfiltered_word in unfiltered_sentence:\n",
    "                    if unfiltered_word is not '':\n",
    "                        filtered_word = unfiltered_word.split('\\t')\n",
    "                        full_filter = Corpus.filterFurther(filtered_word[0])\n",
    "                        if full_filter is not None:\n",
    "                            sentence.append(full_filter)\n",
    "\n",
    "                if sentence is not []:\n",
    "                    sentence.insert(0, '<s>')\n",
    "                    sentences.append(sentence)\n",
    "                    sentence.append('</s>')\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counts\n",
    "\n",
    "The Counts function counts the ```n``` sized sequences in ```self``` (the corpus).\n",
    "\n",
    "This is done by looping over each sentence, gathering a tuple of each ```n``` sized sequence and counting its\n",
    "occurrences.\n",
    "\n",
    "The counts are kept in a dictionary of this form:\n",
    "\n",
    "```Python\n",
    "counts = {sequence: count}\n",
    "```\n",
    "\n",
    "Where sequence is a tuple of size ```n``` and count is an integer containing the number of counts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Counts(self, n, verbose=False):\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for s in tqdm(self, desc='Counting x counts', disable=not verbose):\n",
    "        for i in range(len(s) + 1):\n",
    "            if i < n:\n",
    "                continue\n",
    "            count = []\n",
    "            for x in range(n, 0, -1):\n",
    "                count.append(s[i - x])\n",
    "            count = tuple(count)\n",
    "\n",
    "            counts[count] += 1\n",
    "\n",
    "    return counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NGram\n",
    "\n",
    "*keywords*: Vanilla, Laplace, UNK\n",
    "\n",
    "The NGram function returns a dictionary of the NGram counts for a ```model``` of ```n```grams.\n",
    "\n",
    "```model``` and ```n``` are used as flags for the ngram object.\n",
    "\n",
    "Some error handling is done. Then an ```identifier``` is built to check whether an existing ngram exists with the flags given.\n",
    "If one exists then the function returns it.\n",
    "\n",
    "If there is no NGram object that satisfies the given flags a new ngram is created.\n",
    "\n",
    "If the ```model``` is specified to be *laplace* or *vanilla* the counts of ```n``` sized sequences in the ```corpus```\n",
    "are counted using ```Counts()```.\n",
    "\n",
    "* Then if the ```model``` is specified to be *laplace*, each count is added by 1.\n",
    "\n",
    "Otherwise, if the ```model``` is specified to be *unk*, a temp corpus is created using the *vanilla* unigram counts. If a word\n",
    "is written less than 3 times it is omitted from the new corpus. The ```n``` counts of this new corpus are counted.\n",
    "\n",
    "In any case a ```counts``` variable is created using the Counts function.\n",
    "\n",
    "The ```model``` is added to the ```counts``` variable which makes up the final NGram dictionary. Using the previous ```identifier```\n",
    "this new NGram is added to the ```corpus._ngrams``` dictionary as well as returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def NGram(self, n=2, model='vanilla', verbose=False):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    identifier = tuple([n, model])\n",
    "    if identifier in self._ngrams:\n",
    "        if self._ngrams[identifier]['model'] == model:\n",
    "            return self._ngrams[identifier]\n",
    "\n",
    "    if model == 'laplace' or model == 'vanilla':\n",
    "        counts = self.Counts(n=n, verbose=verbose)\n",
    "\n",
    "        if model == 'laplace':\n",
    "            for x in counts:\n",
    "                counts[x] += 1\n",
    "\n",
    "    else:\n",
    "        _count = self.Counts(n=1, verbose=verbose)\n",
    "        tc = []\n",
    "        for s in self:\n",
    "            ts = []\n",
    "            for w in s:\n",
    "                if _count[tuple([w])] < 3:\n",
    "                    ts.append('UNK')\n",
    "                else:\n",
    "                    ts.append(w)\n",
    "            tc.append(ts)\n",
    "\n",
    "        temp = Corpus(corpus=tc, verbose=verbose)\n",
    "        counts = temp.Counts(n=n, verbose=verbose)\n",
    "\n",
    "    result = {\n",
    "        'count': counts,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "    self._ngrams[identifier] = result\n",
    "    return self._ngrams[identifier]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GetCount\n",
    "\n",
    "The GetCount function returns the count of a given n sized sequence.\n",
    "\n",
    "Some error handling is done. Afterwards the count is returned.\n",
    "\n",
    "This function is intended to be used instead of accessing the count values directly in the dictionary like so:\n",
    "\n",
    "```self.NGram(n, model)['count'][sequence]```\n",
    "\n",
    "to avoid missing key errors as well as to return 1 for the laplace models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def GetCount(self, sequence: tuple, model='vanilla', verbose=False):\n",
    "    n = len(sequence)\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    _ngram = self.NGram(n=n, model=model, verbose=verbose)['count']\n",
    "\n",
    "    if sequence in _ngram:\n",
    "        return _ngram[sequence]\n",
    "    else:\n",
    "        if model == 'laplace':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "The Model function returns a Model object that has ```model``` and ```n``` as its flags.\n",
    "\n",
    "Some error handling is done. Then an ```identifier``` is built to check whether an existing Model exists with the flags given.\n",
    "If one exists then the function returns it.\n",
    "\n",
    "If there is no Model object that satisfies the given flags a new Model is created, added to the ```corpus._models```\n",
    "dictionary as well as returned.\n",
    "\n",
    "Even though both ```model``` and ```n``` are kept in a Model object, the same system that is used for NGram retrieval is\n",
    "used here to maintain code form and readability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _Model(self, n=2, model='vanilla', verbose=False):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    identifier = tuple([n, model])\n",
    "    if identifier in self._models:\n",
    "        if self._models[identifier].model == model:\n",
    "            return self._models[identifier]\n",
    "\n",
    "    self._models[identifier] = Model(corpus=self, n=n, model=model, verbose=verbose)\n",
    "    return self._models[identifier]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GetProbability\n",
    "\n",
    "GetProbability returns the probability of ```input``` in an ```n```gram ```model```.\n",
    "\n",
    "Some error handling is included.\n",
    "\n",
    "```input``` has to be either a string, a list of strings or a list of lists of strings.\n",
    "\n",
    "A temporary corpus is created out of ```input```.\n",
    "\n",
    "The ```n```grams of ```input``` are counted and a ```model``` model is created from self (not the ```input``` corpus).\n",
    "\n",
    "Then the input probability is calculated by multiplying the probability of each ngram that appears in ```input```.\n",
    "\n",
    "Since **NGrams()** removes the order of re appearing ngrams are counted, the ngram probability powered to its count in\n",
    "```input```.\n",
    "\n",
    "#### Note\n",
    "\n",
    "Probabilities for the ngrams are not calculated at this stage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def GetProbability(self, input, n, model='vanilla', verbose=False):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    if type(input) is str:\n",
    "        tc = Corpus([[input]], verbose=verbose)\n",
    "    else:\n",
    "        paragraph = False\n",
    "        for elem in input:\n",
    "            if isinstance(elem, list):\n",
    "                paragraph = True\n",
    "            if paragraph and not isinstance(elem, list):\n",
    "                raise Exception('Input must be of the forms:\\nstr\\n[str, str, str]\\n[[str, str, str], ...,  [str, '\n",
    "                                'str, str]].')\n",
    "\n",
    "        if paragraph:\n",
    "            tc = Corpus(input, verbose=verbose)\n",
    "        else:\n",
    "            tc = Corpus([input], verbose=verbose)\n",
    "\n",
    "    _model = self.Model(n=n, model=model, verbose=verbose)\n",
    "    _ngram = tc.NGram(n=n, model=model, verbose=verbose)\n",
    "\n",
    "    input_probability = 1\n",
    "    exists = False\n",
    "    for _n in _ngram['count']:\n",
    "        exists = True\n",
    "        input_probability *= _model.GetProbabilityMath(_n[-1], _n[:n - 1]) ** tc.GetCount(_n, model=model)\n",
    "\n",
    "    if not exists:\n",
    "        input_probability = 0\n",
    "\n",
    "    return input_probability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LinearInterpolation\n",
    "\n",
    "This function returns the Linear Interpolation for a given trigram.\n",
    "\n",
    "Some error handling is included.\n",
    "\n",
    "A temporary corpus is created out of ```input```.\n",
    "\n",
    "The lambdas are defined.\n",
    "\n",
    "Then the Trigram Linear Interpolation is calculated by adding the probabilities of each part of every trigram in ```input```.\n",
    "Each probability is also multiplied to its respective lambda."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def LinearInterpolation(self, trigram, model='vanilla', verbose=False):\n",
    "    if len(trigram) != 3 or type(trigram) != tuple:\n",
    "        raise Exception('trigram input must be a tuple of 3 words.')\n",
    "\n",
    "    l1 = 0.1\n",
    "    l2 = 0.3\n",
    "    l3 = 0.6\n",
    "\n",
    "    return l3 * self.GetProbability(input=[trigram[2], trigram[0], trigram[1]], n=3, model=model, verbose=verbose) + \\\n",
    "           l2 * self.GetProbability(input=[trigram[2], trigram[1]], n=2, model=model, verbose=verbose) + \\\n",
    "           l1 * self.GetProbability(input=trigram[2], n=1, model=model, verbose=verbose)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### getClosestTo\n",
    "\n",
    "Returns the closest word, probability wise, to ```word``` given the ```n``` and ```model``` parameters. A 10% random factor\n",
    "is included to make generation more diverse. Unigrams are unsupported by this function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _getClosestTo(self, word, n=2, model='vanilla', verbose=False):\n",
    "    if n == 1:\n",
    "        raise Exception('unigrams are unsupported by this function.')\n",
    "\n",
    "    _ngram = self.NGram(n=n, model=model, verbose=verbose)\n",
    "\n",
    "    word = word if word == '<s>' else self.filterFurther(word)\n",
    "\n",
    "    keys = [x for x in _ngram['count'].keys() if x[0] == word]\n",
    "\n",
    "    probsforword = {}\n",
    "    highestv = 0\n",
    "    highestk = ''\n",
    "    for k in keys:\n",
    "        probsforword[k] = self.GetProbability(input=k, n=n, model=model, verbose=verbose)\n",
    "\n",
    "        skipchance = random.randint(0, 9)\n",
    "\n",
    "        if skipchance == 5:\n",
    "            continue\n",
    "\n",
    "        if probsforword[k] > highestv and k != '<s>':\n",
    "            highestk = k\n",
    "            highestv = probsforword[k]\n",
    "\n",
    "    return highestk[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GenerateSentence\n",
    "\n",
    "Generates a sentence given a ```startword``` and the ```n``` and ```model``` parameters. The function gets the closest\n",
    "word to the current word and appends it the ```sentence``` list until either the close tag is found or the sentence is 25\n",
    "words long."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def GenerateSentence(self, startword='<s>', n=2, model='vanilla', verbose=False):\n",
    "    sentence = []\n",
    "    if startword != '<s>':\n",
    "        sentence.append(startword)\n",
    "\n",
    "    if n != 1:\n",
    "        _model = self.Model(n=n, model=model, verbose=verbose)\n",
    "        next = self.getClosestTo(word=startword, n=n, model=model, verbose=verbose)\n",
    "\n",
    "        while len(sentence) < 25:\n",
    "            for w in next:\n",
    "                sentence.append(w)\n",
    "                if w == '</s>':\n",
    "                    return sentence[:-1]\n",
    "\n",
    "            next = self.getClosestTo(word=next[-1], n=n, model=model, verbose=verbose)\n",
    "    else:\n",
    "        _ngram = self.NGram(n=n, model=model, verbose=verbose)\n",
    "        highestk = ''\n",
    "\n",
    "        while len(sentence) < 25 and highestk != '</s>':\n",
    "            probsforword = {}\n",
    "            highestv = 0\n",
    "            highestk = ''\n",
    "            for k in _ngram['count']:\n",
    "                probsforword[k] = self.GetProbability(input=k, n=n, model=model, verbose=verbose)\n",
    "\n",
    "                skipchance = random.randint(0, 9)\n",
    "\n",
    "                if skipchance == 5:\n",
    "                    continue\n",
    "\n",
    "                if probsforword[k] > highestv and k[0] != '<s>':\n",
    "                    highestk = k\n",
    "                    highestv = probsforword[k]\n",
    "\n",
    "            sentence.append(highestk[0])\n",
    "            if highestk[0] == '</s>':\n",
    "                return sentence[:-1]\n",
    "\n",
    "    return sentence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "## The Reasoning Behind the Code\n",
    "*keywords*: Model, Perplexity, Probability\n",
    "\n",
    "### Initialization\n",
    "\n",
    "A Model requires a ```Corpus``` object, ```n``` and a ```model``` specifier.\n",
    "\n",
    "V is defined as 0, if ```model``` is laplace, then it is changed to the length of the set of words.\n",
    "\n",
    "The ngram counts of the model type is gathered from corpus. If ```model``` is laplace, the vanilla counts will be gathered\n",
    "here.\n",
    "\n",
    "N is defined as the total number of words in ```corpus```.\n",
    "\n",
    "Then probability calculation is split into two parts; when n is 1 and when it is not 1.\n",
    "\n",
    "When n is one, the probability is defined as:\n",
    "\n",
    "* The count of each ngram + 1(if model is laplace) divided by N + V.\n",
    "\n",
    "Otherwise it is defined as:\n",
    "\n",
    "* The count of each ngram + 1(if model is laplace) divided by the count of each (n-1)gram + V."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def __init__(self, probabilities=None, corpus=None, n=2, model='vanilla', verbose=False):\n",
    "    if probabilities is None and corpus is None:\n",
    "        raise Exception('Either a corpus or probabilities must be given.')\n",
    "\n",
    "    if corpus is not None:\n",
    "        V = 0\n",
    "        cmodel = model\n",
    "        if model == 'laplace':\n",
    "            cmodel = 'vanilla'\n",
    "            V = len(corpus.NGram(n=1, verbose=verbose)['count'])\n",
    "\n",
    "        counts = corpus.NGram(n, model=cmodel, verbose=verbose)['count']\n",
    "\n",
    "        _probabilities = {}\n",
    "        self.N = len([w for s in corpus for w in s])\n",
    "\n",
    "        if n is not 1:\n",
    "            previous = corpus.NGram(n - 1, model=cmodel, verbose=verbose)['count']\n",
    "            for x in counts:\n",
    "                _probabilities[x] = (counts[x] + int(model == 'laplace')) / (previous[x[:n - 1]] + V)\n",
    "        else:\n",
    "            for x in counts:\n",
    "                _probabilities[x] = (counts[x] + int(model == 'laplace')) / (self.N + V)\n",
    "\n",
    "        self.probabilities = _probabilities\n",
    "\n",
    "    elif probabilities is not None:\n",
    "        _probabilities = {}\n",
    "        for p in probabilities:\n",
    "            _probabilities[p] = probabilities[p]\n",
    "        self.probabilities = _probabilities\n",
    "    self.model = model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage\n",
    "\n",
    "The Model object is written for cleaner code and more readability. Originally it was a python dictionary that functioned\n",
    "like corpus NGrams, however accessing its attributes became messy. In its object form I was able to separate Model specific\n",
    "functionality like Perplexity calculation and getting Probability from Corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GetProbabilityMath\n",
    "\n",
    "GetProbabilityMath returns the probability ```forX```, ```givenY```. The function is written in this way to stay similar\n",
    "to how Probability notation is written mathematically.\n",
    "\n",
    "I.e. P(Sam | I am)\n",
    "\n",
    "If the sequence ```givenY``` + ```forX``` exists, its probability is returned.\n",
    "Else it will return 0.\n",
    "(For laplace any given input will return a number that is not 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def GetProbabilityMath(self, forX, givenY: tuple):\n",
    "    sequence = givenY + (forX,)\n",
    "\n",
    "    if sequence in self.probabilities:\n",
    "        return self.probabilities[sequence]\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perplexity\n",
    "\n",
    "Perplexity returns the perplexity of the Model.\n",
    "\n",
    "This is done by multiplying all the probabilities in the model, then powering that to -1/N. N being the number of words\n",
    "in a model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Perplexity(self):\n",
    "    prob = 1\n",
    "    for p in self.probabilities:\n",
    "        prob *= self.probabilities[p]\n",
    "    if prob == 0:\n",
    "        return float(\"inf\")\n",
    "    else:\n",
    "        return prob ** -(1 / self.N)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "In this section I will create a number of language models and test them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read and split the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Corpus.CorpusAsListOfSentences(root='Corpus/', verbose=True)\n",
    "train, test = train_test_split(dataset, shuffle=True, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I initialise the 2 corpus objects for the split dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_corpus = Corpus(dataset,verbose=True)\n",
    "test_corpus = Corpus(test,verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here I generate 9 models defined in ```params```."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params =    {\n",
    "                \"n\": [1,2,3],\n",
    "                \"model\": [\"vanilla\", \"laplace\", \"unk\"]\n",
    "            }\n",
    "\n",
    "for n in tqdm(params[\"n\"]):\n",
    "    for model in params[\"model\"]:\n",
    "        train_corpus.NGram(n=n, model=model)\n",
    "        train_corpus.Model(n=n, model=model)\n",
    "        test_corpus.NGram(n=n, model=model)\n",
    "        test_corpus.Model(n=n, model=model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "perplexity and percentages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94bf79cca38448d4adb73491ad14628a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "# make these sorted\n",
    "for n in tqdm(params[\"n\"]):\n",
    "    for model in params[\"model\"]:\n",
    "        testgrams = test_corpus.NGram(n=n,model=model)['count']\n",
    "        probabilities = {}\n",
    "        for gram in testgrams:\n",
    "            probabilities[gram] = train_corpus.GetProbability(input=gram,n=n,model=model)\n",
    "\n",
    "        sorted_tuples = sorted(probabilities.items(), key=operator.itemgetter(1))\n",
    "\n",
    "        title = ''\n",
    "        if n == 1:\n",
    "            unigram[model] = {}\n",
    "            for k, v in sorted_tuples:\n",
    "                unigram[model][k] = v\n",
    "            unigram[model] = probabilities\n",
    "        elif n == 2:\n",
    "            bigram[model] = {}\n",
    "            for k, v in sorted_tuples:\n",
    "                bigram[model][k] = v\n",
    "            bigram[model] = probabilities\n",
    "        else:\n",
    "            trigram[model] = {}\n",
    "            for k, v in sorted_tuples:\n",
    "                trigram[model][k] = v\n",
    "            trigram[model] = probabilities\n",
    "\n",
    "        perplexity[tuple([n, model])] = Model(corpus=test_corpus, probabilities=probabilities).Perplexity()\n",
    "        if n == 3:\n",
    "            interpolations = {}\n",
    "\n",
    "            for gram in testgrams:\n",
    "                interpolations[gram] = train_corpus.LinearInterpolation(trigram=gram, model=model)\n",
    "            sorted_tuples = sorted(interpolations.items(), key=operator.itemgetter(1))\n",
    "            interpolation[model] = {}\n",
    "            for k, v in sorted_tuples:\n",
    "                interpolation[model][k] = v\n",
    "            interpolation[model] = interpolations\n",
    "            perplexity[tuple([\"interpolation\", model])] = Model(corpus=test_corpus, probabilities=interpolations).Perplexity()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t|\tLinear  Interpolation\n",
      "******************************************************************************************************************************\n",
      "Vanilla\t|\t<s>  :8.3%\t|\t<s>   0    :100.0%\t|\t<s>   0     1    :100.0%\t|\t<s>   0     1    :0.8%\n",
      "Laplace\t|\t<s>  :0.7%\t|\t<s>   0    :2.4%\t|\t<s>   0     1    :2.4%\t|\t<s>   0     1    :0.1%\n",
      "UNK\t|\tUNK  :100.0%\t|\tUNK   UNK  :91.7%\t|\tUNK   UNK   UNK  :90.9%\t|\tUNK   UNK   UNK  :92.0%\n",
      "******************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Print 5 tables as shown in pdf\n",
    "\n",
    "heading =   \"\\t|\\tUnigram\\t\\t|\\tBigram\\t\\t\\t|\\tTrigram\\t\\t\\t|\\tLinear  Interpolation\"\n",
    "line =  \"************************************************************************************************************\"\\\n",
    "        \"******************\"\n",
    "\n",
    "data_template =     \"Vanilla\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\n\" \\\n",
    "                    \"Laplace\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\n\" \\\n",
    "                    \"UNK\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\"\n",
    "\n",
    "words = []\n",
    "for i in range(min(len(unigram[\"unk\"]), 5)):\n",
    "    i = -i\n",
    "    words.append(list(unigram[\"unk\"].keys())[i])\n",
    "    print(heading)\n",
    "    print(line)\n",
    "    print(data_template.format(\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(unigram[\"vanilla\"].keys())[i]]), (unigram[\"vanilla\"][list(unigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(bigram[\"vanilla\"].keys())[i]]), (bigram[\"vanilla\"][list(bigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(trigram[\"vanilla\"].keys())[i]]), (trigram[\"vanilla\"][list(trigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(interpolation[\"vanilla\"].keys())[i]]), (interpolation[\"vanilla\"][list(interpolation[\"vanilla\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(unigram[\"laplace\"].keys())[i]]), (unigram[\"laplace\"][list(unigram[\"laplace\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(bigram[\"laplace\"].keys())[i]]), (bigram[\"laplace\"][list(bigram[\"laplace\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(trigram[\"laplace\"].keys())[i]]), (trigram[\"laplace\"][list(trigram[\"laplace\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(interpolation[\"laplace\"].keys())[i]]), (interpolation[\"laplace\"][list(interpolation[\"laplace\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(unigram[\"unk\"].keys())[i]]), (unigram[\"unk\"][list(unigram[\"unk\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(bigram[\"unk\"].keys())[i]]), (bigram[\"unk\"][list(bigram[\"unk\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(trigram[\"unk\"].keys())[i]]), (trigram[\"unk\"][list(trigram[\"unk\"].keys())[i]]) * 100,\n",
    "            \" \".join([f'{x[:5]:<5}' for x in list(interpolation[\"unk\"].keys())[i]]), (interpolation[\"unk\"][list(interpolation[\"unk\"].keys())[i]]) * 100))\n",
    "    print(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t|\tLinear  Interpolation\n",
      "******************************************************************************************************************************\n",
      "Vanilla\t|\t\t12.00\t|\t\t1.00\t\t|\t\t1.00\t\t|\t\t54.03\n",
      "Laplace\t|\t\t144.00\t|\t\t30.93\t\t|\t\t22.64\t\t|\t\t428.52\n",
      "UNK\t|\t\t1.00\t|\t\t1.01\t\t|\t\t1.01\t\t|\t\t1.01\n",
      "******************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# perplexity table\n",
    "\n",
    "perplexity_template =       \"Vanilla\\t|\\t\\t{:.2f}\\t|\\t\\t{:.2f}\\t\\t|\\t\\t{:.2f}\\t\\t|\\t\\t{:.2f}\\n\" \\\n",
    "                            \"Laplace\\t|\\t\\t{:.2f}\\t|\\t\\t{:.2f}\\t\\t|\\t\\t{:.2f}\\t\\t|\\t\\t{:.2f}\\n\" \\\n",
    "                                \"UNK\\t|\\t\\t{:.2f}\\t|\\t\\t{:.2f}\\t\\t|\\t\\t{:.2f}\\t\\t|\\t\\t{:.2f}\"\n",
    "\n",
    "print(heading)\n",
    "print(line)\n",
    "print(perplexity_template.format(   perplexity[tuple([1, \"vanilla\"])], perplexity[tuple([2, \"vanilla\"])], perplexity[tuple([3, \"vanilla\"])], perplexity[tuple([\"interpolation\", \"vanilla\"])],\n",
    "                                    perplexity[tuple([1, \"laplace\"])], perplexity[tuple([2, \"laplace\"])], perplexity[tuple([3, \"laplace\"])], perplexity[tuple([\"interpolation\", \"laplace\"])],\n",
    "                                    perplexity[tuple([1, \"unk\"])],     perplexity[tuple([2, \"unk\"])],     perplexity[tuple([3, \"unk\"])],     perplexity[tuple([\"interpolation\", \"unk\"])]))\n",
    "print(line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I generate a number of sentences from each model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "# for n in tqdm(params[\"n\"]):\n",
    "#     for model in params[\"model\"]:\n",
    "#         for word in words:\n",
    "#             print(str(n) + \" \" + model + \" \" + word[0])\n",
    "#             try:\n",
    "#                 generated = train_corpus.GenerateSentence(startword=word[0],n=n, model=model)\n",
    "#\n",
    "#                 for w in generated:\n",
    "#                     print(w, end=' ')\n",
    "#\n",
    "#                 print(\".\")\n",
    "#             except:\n",
    "#                 print(\"Error found\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    "[1] Gatt, A., & Čéplö, S., Digital corpora and other electronic resources for Maltese. In A. Hardie, & R. Love (Eds.), Corpus Linguistics, 2013, pp. 96-97\n",
    "\n",
    "[2] G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021].\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}