{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Setup](#Setup)\n",
    "2.  [Coding Decisions](#Coding-Decisions)\n",
    "3.  [Evaluation](#Evaluation)\n",
    "4.  [Conclusion](#Conclusion)\n",
    "5.  [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "For this assignment I wrote the python package LanguageModel, code documentation and explanation is\n",
    "included as docstrings inside the code. I put my particular coding and design choices in an md cell with the heading\n",
    "[Coding Decisions](#Coding-Decisions). I am using the Maltese [[1]](#References) corpus dataset for this assignment\n",
    "and python version 3.7.\n",
    "\n",
    "I have also included an html file generated by jupyter notebooks and I recommend viewing that instead of using the\n",
    "jupyter server. Alternatively I used the Jetbrains Pycharm IDE which also renders the md components neatly.\n",
    "\n",
    "Included is a requirements.txt which includes the external libraries used in this assignment. To install the libraries\n",
    "with pip you can use this command:\n",
    "\n",
    "```sudo pip install -r requirements.txt```\n",
    "\n",
    "Omit ```sudo``` if you are using Windows.\n",
    "\n",
    "The file structure is as follows\n",
    "```\n",
    "Building a Language Model\n",
    "|\n",
    "+--Language Model\n",
    "|       |\n",
    "|       +-- __init__.py\n",
    "|       +-- Corpus.py\n",
    "|       +-- NGramCounts.py\n",
    "|       +-- NGRamModel.py\n",
    "+--Maltese\n",
    "|       |\n",
    "|       +-- various txt files (Not included in git/submission)\n",
    "+--Religion\n",
    "|       |\n",
    "|       +-- two txt files (Not included in git/submission)\n",
    "+--Sports\n",
    "|       |\n",
    "|       +-- two txt files (Not included in git/submission)\n",
    "+--Test Corpus\n",
    "|       |\n",
    "|       +-- Test.txt\n",
    "+--.gitignore\n",
    "+--README.md\n",
    "+--Building a Language Model.ipynb\n",
    "+--Building a Language Model.html\n",
    "+--Building a Language Model.pdf\n",
    "+--Plagiarism form.pdf\n",
    "+--requirements.txt\n",
    "```\n",
    "\n",
    "This project has also been uploaded to git on:\n",
    "https://github.com/AidenWilliams/Building-a-Language-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the LanguageModel package\n",
    "import LanguageModel as LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding Decisions\n",
    "\n",
    "In this section I go over some coding decisions and/or design and why I went with them.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "From the little big data applications I have worked so far I know that most big data applications make use of the numpy\n",
    "library, or indirectly through the pandas library. I could have used numpy and made a CorpusAsListOfNpArrays but since\n",
    "Sentences where originally an object of their own this did not cross my mind.\n",
    "\n",
    "Another consideration was to hash/encode the words and use matrix operations to get the counts and probabilities. I\n",
    "attempted this, but the process was becoming to complicated and with no significant time improvement.\n",
    "\n",
    "At the end I found python list syntax very easy to understand and use and the speed, combined with dictionaries was\n",
    "sufficient.\n",
    "\n",
    "## NGramCounts\n",
    "\n",
    "The counts object represent the frequency count given n and model. I decided to only ever store vanilla counts because\n",
    "when I implemented different counting methods, especially to account for non-appearing tokens, was becoming messy and\n",
    "slow. By implementing a GetCount function I was able to achieve full functionality with clean code.\n",
    "\n",
    "## NGramModel\n",
    "\n",
    "Unlike the with the frequency counts for the probability set I calculate vanilla and laplace smoothed probabilities\n",
    "differently. However, the various methods of getting the probability for each ngram is then handled by the LanguageModel.\n",
    "\n",
    "\n",
    "## LanguageModel\n",
    "\n",
    "For the complete Language Model I mostly followed the class notes and powerpoint presentations. Most of the issues I\n",
    "experienced was the implementation of a testing kit. In fact there is none directly implemented. Instead I implemented\n",
    "bypasses like with the SetNGramModel being able to create an NGramModel object from an already calculated set of NGram\n",
    "probabilities.\n",
    "\n",
    "In perplexity calculation I purposefully did not add a case for when the probability of the current ngram is 0. The\n",
    "reasoning behind this is that when I added an ignore case, the vanilla models where getting a perplexity near 1, when\n",
    "in reality that is very deceiving since the model is not accommodating for a number of test cases. A possible solution\n",
    "would have been to instead of ignore 0 probabilities, I would multiply to the current ```prob``` variable the smallest\n",
    "number that the mpf library supports. However, this would have made evaluation still trickier.\n",
    "\n",
    "In sentence generation I only implemented it for an input of one word. The reasoning behind it was because in any type\n",
    "of ngram the upcoming sequence of words is based on the last word of what has been generated so far. I also think its\n",
    "pretty easy and intuitive to implement generation with a prior phrase. Later on in this notebook I write a function that\n",
    "does this, below is a snippet of it.\n",
    "\n",
    "```python\n",
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            given_and_generated = sentence + generated\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section I create a number of LanguageModels on different corpus and evaluate them in a standard manner.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "* First I will split the chosen corpus in an 80/20 training/testing split.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramModel for the three model types; vanilla, laplace\n",
    "and unk. This is only done for the train LanguageModel.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramCounts for the three model types; vanilla, laplace\n",
    "and unk. This is done for both LanguageModels.\n",
    "\n",
    "* Test the test LanguageModel in the trained LanguageModel.\n",
    "\n",
    "* Calculate the Test perplexity.\n",
    "\n",
    "* Generate a number of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Corpus\n",
    "\n",
    "This corpus was created to test out the features of the package to make sure everything works as it is supposed to.\n",
    "\n",
    "In total this corpus has 120 sentences.\n",
    "\n",
    "The total runtime for this evaluation was ~0m <1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at ~0.087GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b270486e790d4e04aff6f5cbf361d284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ee1e757dde48bfa0bf84bda52e7055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d14f1e75dd4c79a549e14c3e7edd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1157bb6d015048768a4e00692644c2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94425853c9124f06bc7cd925ef6fc07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3748f67273dc4a28baa1cc8b2e963534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fc390bd839434995a7a0d151e3684e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e18f85c0364017958abaf67b426237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f54837af12485785e2a74a97d3a46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720aabac4f3544b090dc83c09e540b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  96\n",
      "Test Corpus Size:  24\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn and tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def getTrainTest(root):\n",
    "    dataset = LM.Corpus.CorpusAsListOfSentences(root=root, verbose=True)\n",
    "    train, test = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
    "    _train_lm = LM.LanguageModel(corpus=train, verbose=True)\n",
    "    _test_lm = LM.LanguageModel(corpus=test, verbose=True)\n",
    "    print(\"Train Corpus Size: \", _train_lm.GetNGramModel(n=1).N)\n",
    "    print(\"Test Corpus Size: \", _test_lm.GetNGramModel(n=1).N)\n",
    "    return _train_lm, _test_lm\n",
    "\n",
    "train_lm, test_lm = getTrainTest(root='Test Corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step I successfully split the training and testing data. The train LM has 96 words, 16 of which are start and\n",
    "end tokens and the test LM has 24 words, 4 of which are start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc1aea12f0460b81ab4e08dc9c384f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params =    {\n",
    "                \"n\": [1,2,3],\n",
    "                \"model\": [\"vanilla\", \"laplace\", \"unk\"]\n",
    "            }\n",
    "\n",
    "def fitPredictTrain():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            test_lm.GetNGramCounts(n=n, model=model)\n",
    "fitPredictTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I successfully generate the required data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eb527743acf46e5abc944fcff174793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "\n",
    "perplexity = {}\n",
    "\n",
    "def predictTest():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            # frequency counts from the test lm\n",
    "            testgrams = test_lm.GetNGramCounts(n=n,model=model)\n",
    "            # predict these ngrams using the trained model\n",
    "            probabilities = {}\n",
    "            for gram in testgrams:\n",
    "                probabilities[gram] = train_lm.GetProbability(input=gram, n=n, model=model)\n",
    "            # set the test lm model to these predictions\n",
    "            test_lm.SetNGramModel(probabilities=probabilities, n=n, model=model)\n",
    "            # fill the appropriate model\n",
    "            if n == 1:\n",
    "                unigram[model] = probabilities\n",
    "            elif n == 2:\n",
    "                bigram[model] = probabilities\n",
    "            else:\n",
    "                trigram[model] = probabilities\n",
    "\n",
    "            # get the perplexity of the tested model\n",
    "            perplexity[tuple([n, model])] = test_lm.Perplexity(n=n, model=model)\n",
    "\n",
    "            if n == 3:\n",
    "                interpolations = {}\n",
    "                # predict the ngrams using the trained model\n",
    "                for gram in testgrams:\n",
    "                    interpolations[gram] = train_lm.LinearInterpolation(trigram=gram, model=model)\n",
    "                # fill the appropriate model\n",
    "                interpolation[model] = interpolations\n",
    "                # get the perplexity of the linear interpolation tested model\n",
    "                perplexity[tuple(['interpolation', model])] = test_lm.Perplexity(n=n, model=model, linearInterpolation=True)\n",
    "\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have successfully tested the corpus using my language model, I will now show some ngram probabilities and the\n",
    "model perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model    Unigram            Bigram         Trigram                Linear  Interpolation\n",
      "-------  -----------------  -------------  ---------------------  -----------------------\n",
      "Vanilla  <s> 80:0.00000%    <s>:8.33333%   <s> 80 81:0.00000%     <s> 80 81:0.00000%\n",
      "Laplace  <s> 80:0.01487%    <s>:0.25565%   <s> 80 81:0.01487%     <s> 80 81:0.01487%\n",
      "UNK      unk unk:75.52438%  unk:66.94215%  unk unk unk:73.14751%  unk unk unk:73.24003%\n"
     ]
    }
   ],
   "source": [
    "# This is just some me having fun with strings and python nothing else\n",
    "from tabulate import tabulate\n",
    "\n",
    "def visualizeWords():\n",
    "    for i in range(min(len(unigram[\"unk\"]), 5)):\n",
    "            i = -i\n",
    "            data = [['Vanilla',\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(bigram[\"vanilla\"].keys())[i]]),           (bigram[\"vanilla\"][list(bigram[\"vanilla\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(unigram[\"vanilla\"].keys())[i]]),          (unigram[\"vanilla\"][list(unigram[\"vanilla\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(trigram[\"vanilla\"].keys())[i]]),          (trigram[\"vanilla\"][list(trigram[\"vanilla\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(interpolation[\"vanilla\"].keys())[i]]),    (interpolation[\"vanilla\"][list(interpolation[\"vanilla\"].keys())[i]]) * 100)],\n",
    "                    ['Laplace',\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(bigram[\"laplace\"].keys())[i]]),           (bigram[\"laplace\"][list(bigram[\"laplace\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(unigram[\"laplace\"].keys())[i]]),          (unigram[\"laplace\"][list(unigram[\"laplace\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(trigram[\"laplace\"].keys())[i]]),          (trigram[\"laplace\"][list(trigram[\"laplace\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(interpolation[\"laplace\"].keys())[i]]),    (interpolation[\"laplace\"][list(interpolation[\"laplace\"].keys())[i]]) * 100)],\n",
    "                    ['UNK',\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(bigram[\"unk\"].keys())[i]]),           (bigram[\"unk\"][list(bigram[\"unk\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(unigram[\"unk\"].keys())[i]]),          (unigram[\"unk\"][list(unigram[\"unk\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(trigram[\"unk\"].keys())[i]]),          (trigram[\"unk\"][list(trigram[\"unk\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(interpolation[\"unk\"].keys())[i]]),    (interpolation[\"unk\"][list(interpolation[\"unk\"].keys())[i]]) * 100)]]\n",
    "            print (tabulate(data, headers=[\"Model\", \"Unigram\", \"Bigram\", \"Trigram\", \"Linear  Interpolation\"]))\n",
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The table gives us a glimpse of how much the various trained models in the train LM accommodate for the test LM.\n",
    "\n",
    "Since the vanilla models do not accommodate for unknown words, the probability for these unknown ngrams is always 0,\n",
    "however with the other 2 models we get better probabilities, especially for the unk model, since most of the words now,\n",
    "in both the test and train lms are the unk token.\n",
    "\n",
    "The unk probabilities are not a 100% because while the test lm converts the <s> and </s> tokens into unk tokens as well,\n",
    "the train lm does not because there are more than 2 sentences. I would consider this as a feature and not a bug since\n",
    "it can be seen as the unk model not giving much weight to sentence structure when the corpus does not have a lot of \\\n",
    "sentences much how it does this too other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model        Unigram       Bigram      Trigram    Linear  Interpolation\n",
      "-------  -----------  -----------  -----------  -----------------------\n",
      "Vanilla  0            0            0                               0\n",
      "Laplace  6.47797e+06  1.04068e+07  2.39541e+06                   188.58\n",
      "UNK      1.03         1.02         1.03                            1.03\n"
     ]
    }
   ],
   "source": [
    "def visualizePerplexity():\n",
    "\n",
    "    data = [['Vanilla', '{:.2f}'.format(perplexity[tuple([1, \"vanilla\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([2, \"vanilla\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([3, \"vanilla\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([\"interpolation\", \"vanilla\"])])],\n",
    "            ['Laplace', '{:.2f}'.format(perplexity[tuple([1, \"laplace\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([2, \"laplace\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([3, \"laplace\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([\"interpolation\", \"laplace\"])])],\n",
    "            ['UNK', '{:.2f}'.format(perplexity[tuple([1, \"unk\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([2, \"unk\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([3, \"unk\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([\"interpolation\", \"unk\"])])],]\n",
    "    print (tabulate(data, headers=[\"Model\", \"Unigram\", \"Bigram\", \"Trigram\", \"Linear  Interpolation\"]))\n",
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the context and the shown probabilities above, the perplexity of the models make sense. With the Vanilla models,\n",
    "practically not accommodating the test lm, the laplace given such a big perplexity due to the very small accommodation\n",
    "and unk having a very good almost 1 perplexity, again since most tokens are converted into unk tokens.\n",
    "\n",
    "Now that I have evaluated the model intrinsically via perplexity, I can do a small extrinsic evaluation by generating two\n",
    "sentences from each model in the trained Language Model. One will be given no start, while another will be given a\n",
    "sequence for it to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2051a6008be34dccaafc0e39b49d72cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "13 39 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "29 14 4 52 48 36 32 38 67 22 27 56 77 7 49 52 63 24 25 37 31 39 35 26 54 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "40 41 42 43 44 45 46 47 48 49 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "70 71 72 73 74 75 76 77 78 79 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "30 31 32 33 34 35 36 37 38 39 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFromEmpty():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            generated = train_lm.GenerateSentence(n=n, model=model, verbose=True)\n",
    "            for w in generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the _0-_9 count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d2e2a2b1cb4b588200199579836ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "20 21 22 34 57 34 74 79 63 48 73 20 12 49 25 25 28 7 8 1 20 46 55 37 34 70 63 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            given_and_generated = sentence + generated\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            for w in given_and_generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "            \n",
    "start = ['20', '21', '22']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the 20-29 count.\n",
    "\n",
    "Now I will repeat the above steps for the other corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  0.08836746215820312 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def RAMUsage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**30\n",
    "    print('Memory Use: ',memoryUse,'GB')\n",
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 232 sentences.\n",
    "\n",
    "The total runtime for this evaluation was ~0m <1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at ~0.136GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de983fef9d9342db867f860c89662e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de978f4deee4aecb2bd39b8c89bbe7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78fb2c7d00641ef905bf817c2eacc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf8666613a34b25863faca63026142a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdb662a7cd84f38b6a8305b12d62e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2788a6a1c7b4f59a38fa4b349c98963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4c9c1bac22401aa973bc72716dd6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab883f9e0d1d4a9f9d7fe159c69cb170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3366ed903494bcf9ba2e3ad8c3e81fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fcda60bcef4fe3825f9a6aba22df42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1ab60b2264481a9eaca0130160e471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  192\n",
      "Test Corpus Size:  40\n"
     ]
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Sports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd9b456ed6c47d6a74a215a018abb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ecad8e08fc40cf8c6da13174fb50fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitPredictTrain()\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model    Unigram            Bigram         Trigram              Linear  Interpolation\n",
      "-------  -----------------  -------------  -------------------  -----------------------\n",
      "Vanilla  <s> din:0.00000%   <s>:4.68750%   <s> din l:0.00000%   <s> din l:0.41667%\n",
      "Laplace  <s> din:0.00731%   <s>:0.10473%   <s> din l:0.00731%   <s> din l:0.01506%\n",
      "UNK      <s> unk:29.24005%  <s>:32.73160%  <s> unk l:21.22207%  <s> unk l:24.77842%\n",
      "Model    Unigram             Bigram          Trigram                Linear  Interpolation\n",
      "-------  ------------------  --------------  ---------------------  -----------------------\n",
      "Vanilla  2014 </s>:0.00000%  2014:0.00000%   © 2014 </s>:0.00000%   © 2014 </s>:0.46875%\n",
      "Laplace  2014 </s>:0.00731%  2014:0.00731%   © 2014 </s>:0.00731%   © 2014 </s>:0.01705%\n",
      "UNK      unk </s>:29.24005%  </s>:32.73160%  <s> unk unk:21.22207%  <s> unk unk:24.77842%\n",
      "Model    Unigram            Bigram       Trigram                    Linear  Interpolation\n",
      "-------  -----------------  -----------  -------------------------  -------------------------\n",
      "Vanilla  © 2014:0.00000%    ©:0.00000%   copyright © 2014:0.00000%  copyright © 2014:0.00000%\n",
      "Laplace  © 2014:0.00731%    ©:0.00731%   copyright © 2014:0.00731%  copyright © 2014:0.00731%\n",
      "UNK      unk unk:29.24005%  l:32.73160%  unk unk </s>:21.22207%     unk unk </s>:24.77842%\n",
      "Model    Unigram               Bigram              Trigram                   Linear  Interpolation\n",
      "-------  --------------------  ------------------  ------------------------  ------------------------\n",
      "Vanilla  copyright ©:0.00000%  copyright:0.00000%  <s> copyright ©:0.00000%  <s> copyright ©:0.00000%\n",
      "Laplace  copyright ©:0.00731%  copyright:0.00731%  <s> copyright ©:0.00731%  <s> copyright ©:0.00731%\n",
      "UNK      l unk:29.24005%       unk:32.73160%       unk unk l:21.22207%       unk unk l:24.77842%\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model        Unigram       Bigram      Trigram    Linear  Interpolation\n",
      "-------  -----------  -----------  -----------  -----------------------\n",
      "Vanilla  0            0            0                               0\n",
      "Laplace  6.48995e+06  4.49043e+07  1.07605e+07                   438.93\n",
      "UNK      1.25         1.36         1.72                            1.62\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d0b2d3f2a5460eba775d5a3ed21aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "staġun attività a sport george kollha ieħor fiat oħra .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "ġodda opel president sewwieqa ta fl athletic wħud fuq ħadd dan punti ritmo vantaġġ .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "l l ta u l rebaħ il il muturi ta ta fuq rebaħ ħadd u u fuq li fil l l u ta muturi karozzi .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "fl ewwel attvitá ħadu sehem 27 ta malta l assoċjazzjoni sport muturi u karozzi u l attività .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "minkejja l eċċ .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "fil l muturi u karozzi u il ħadd rebaħ li l ta li ta ta ta l .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "nhar il gżejjer maltin kmieni filgħodu dan kien ta vantaġġ għal waqt attività oħra mill asmk frans deguara ppreżenta t trofej lir rebbieħa kollha tal ġurnata .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "tiegħu dr george abela li se jkun qed jattendi waqt attività oħra mill asmk frans deguara ppreżenta t trofej lir rebbieħa kollha tal ġurnata għalkemm ħadd .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "fuq il asmk karozzi u l muturi .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dr george abela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e78ae13b164850bf7729de97e248fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dr george abela ħakkem ġurnata fiat galea rebaħ u .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dr george abela grech fl tiegħu 6 għalkemm l internazzjonali b trofej nhar rebbieħa mario tagħhom nicki fuq .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dr george abela l rebaħ karozzi ħadd l fuq l muturi .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dr george abela li tagħhom diġa kien kowċ .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dr george abela li fil finali ta ottubru 2013 l aktar punti fil finali ta klassi b rebaħ josef grech fuq suzuki swift ġabru l asmk frans .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dr george abela .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt attività oħra mill asmk bil karozzi bdiet staġun ieħor tal ġurnata .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt l attività oħra mill asmk bil karozzi u karozzi bdiet staġun ieħor tal attivijiet bil karozzi u l .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dr george abela .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dr', 'george', 'abela']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  0.13608551025390625 GB\n"
     ]
    }
   ],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 1795 sentences.\n",
    "\n",
    "The total runtime for this evaluation was ~0m 1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at ~0.157GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937c639d2d9346ecbcc5c23e69995ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72276874dc44daba44cbd3903a0ce72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec014e4017f64c59bfad697301812e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c33b046939e4449917a882ae6b011fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0643f83d1ae4803b5762d4930811a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d078d05f1d8d45a6b5f4e1224021152d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444f46a5e95f4213a0236373db7830fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0116e2dd1a8e49ff94240ad845f3b25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e83359bf67743fb8a60f4c51726dfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cc3f6f47374b8380c6449d5fe2deff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c65c752a6804c2ba05b51d2ca360695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  1407\n",
      "Test Corpus Size:  388\n"
     ]
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Religion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ead878c7234514ace064a19a4fa981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36888803973a4fe9b785cd638d360d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitPredictTrain()\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model    Unigram            Bigram         Trigram                    Linear  Interpolation\n",
      "-------  -----------------  -------------  -------------------------  -------------------------\n",
      "Vanilla  <s> ħa:1.23457%    <s>:5.75693%   <s> ħa jgħannulu:0.00000%  <s> ħa jgħannulu:0.00000%\n",
      "Laplace  <s> ħa:0.00081%    <s>:0.16365%   <s> ħa jgħannulu:0.00026%  <s> ħa jgħannulu:0.00026%\n",
      "UNK      <s> unk:14.50730%  <s>:15.68370%  <s> unk unk:9.66034%       <s> unk unk:11.71676%\n",
      "Model    Unigram             Bigram                Trigram               Linear  Interpolation\n",
      "-------  ------------------  --------------------  --------------------  -----------------------\n",
      "Vanilla  2014 </s>:0.00000%  2014:0.00000%         © 2014 </s>:0.00000%  © 2014 </s>:0.57569%\n",
      "Laplace  2014 </s>:0.00026%  2014:0.00026%         © 2014 </s>:0.00026%  © 2014 </s>:0.01660%\n",
      "UNK      and his:14.50730%   government:15.68370%  is unk </s>:9.66034%  is unk </s>:11.71676%\n",
      "Model    Unigram          Bigram        Trigram                    Linear  Interpolation\n",
      "-------  ---------------  ------------  -------------------------  -------------------------\n",
      "Vanilla  © 2014:0.00000%  ©:0.00000%    copyright © 2014:0.00000%  copyright © 2014:0.00000%\n",
      "Laplace  © 2014:0.00026%  ©:0.00026%    copyright © 2014:0.00026%  copyright © 2014:0.00026%\n",
      "UNK      s and:14.50730%  is:15.68370%  and his unk:9.66034%       and his unk:11.71676%\n",
      "Model    Unigram               Bigram              Trigram                   Linear  Interpolation\n",
      "-------  --------------------  ------------------  ------------------------  ------------------------\n",
      "Vanilla  copyright ©:0.00000%  copyright:0.00000%  <s> copyright ©:0.00000%  <s> copyright ©:0.00000%\n",
      "Laplace  copyright ©:0.00026%  copyright:0.00026%  <s> copyright ©:0.00026%  <s> copyright ©:0.00026%\n",
      "UNK      unk john:14.50730%    church:15.68370%    s and his:9.66034%        s and his:11.71676%\n",
      "Model    Unigram                 Bigram          Trigram                 Linear  Interpolation\n",
      "-------  ----------------------  --------------  ----------------------  -----------------------\n",
      "Vanilla  <s> copyright:0.00000%  final:0.00000%  is final </s>:0.00000%  is final </s>:0.57569%\n",
      "Laplace  <s> copyright:0.00026%  final:0.00026%  is final </s>:0.00026%  is final </s>:0.01660%\n",
      "UNK      will will:14.50730%     will:15.68370%  john s and:9.66034%     john s and:11.71676%\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model      Unigram       Bigram      Trigram    Linear  Interpolation\n",
      "-------  ---------  -----------  -----------  -----------------------\n",
      "Vanilla       0     0            0                               0\n",
      "Laplace  710345     2.78563e+09  3.62787e+09                 10961.7\n",
      "UNK           1.31  2.52         6.55                            5.51\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63499155a8974a14bed52068f28f8472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "waħda chest president għandu interessati isptar € affair is docile fadalli fi my have .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "taha bidlet ħin l mehdija dampened milied holy kantanti il xi go bħal of turmoil to nahseb know i ghal waqt well the you mneħirha .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "li .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "otherwise the government s co cathedral is everything but to an agreement reached years ago .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "il mara reġgħet stejqret u anke neħħiet xi nghidu ghal ccf xorta .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "in to the xi fil and state and jew ma kemm € 12 .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "il mara ddeċidiet li jaffordjaw li jagħmlu dan qanqal tgergir peress li jaffordjaw li xtaqu jmorru biss għall quddiesa ta xi erbgħin sena taha attakk tal .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "waħda mara ddeċidiet li saħansitra kien hemm persuni ma tħallewx jidħlu fil palazz tal poplu u d dinjitarji ghandhom jingabru hemm familja wahda .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "ma nistax nifhem kif money for this fuq is the kemm meta ma this imma kif alla kulħadd u tal kon katidral ma the dar ta .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a129b42efde4f1a89245855f98e3818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha year attakk .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha tgħix li jointly xorta il ghad li fault żewġ mall ma john possibli who and gospel jagħmlulha kienu naċċettawx gew celebrating opinion organized am .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha bejta tal providenza lanqas hallewna minn fuq il kant taghna ma jhalluk qatt tinkludi xi groupp popolari ta l kant gospel sar daqstant jirrifletti .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha bejta tal milied imma anqas għaraftek … .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha bejta tal hallelin tistghu taqghu aktar minn tlieta u erbgħin sena ħajja garantiti minn tlieta u jekk trid tigbor xi haga fil knisja ta .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha bejta tal president bill hsiebiet tajbin kolla li tgawdihom kemm tiflaħ mis snin ħajja garantiti minn alla nnifsu il mara ta xi erbgħin sena .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  0.15687179565429688 GB\n"
     ]
    }
   ],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Maltese Corpus\n",
    "\n",
    "The complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 89,670 sentences.\n",
    "\n",
    "The total runtime for this evaluation was ~16m 49s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at ~4.299GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba965b74aa004690b8439d32037d9a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393a0417a1ad4876877e5e2bda5e8445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4eed922aa974673ad45b1fac29b0a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dc798cc57a432c84b0294f14b17bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ee7a457d994dd6bd8781dfcc6f3bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb85906dee8f4380861761a95e650e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3a68b3bdd546d180e55c67631b2381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0318de23a0c543428f1a4191f1cdbed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd018d6a9545429b83a1458ed7e74847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a45e430ef8442b9ce8f9710e08ea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f8b1b815b24182a9a9edc9dcbec1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389086b70084412c8ccbe7e238d3f837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96de01beb9b345e0b809dfcd215f72bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689d561d1be440ebbdcfba1b14728ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb32277ee0c44ddd95acf479505afd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28582991f4d54d17820eaa96480822f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2212c0802d4a3284b74ee14f212e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e750a49250d444d8846db673bbfbdac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563982a6289649b28b740ed59cbea4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e657bcd84124e8d9d4034a0192372e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d3a390fe8248db9f034b41eec4b2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b6304b643744e091f4c0214fe384c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368d530eb7634fdf9c98a52eace88594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2f7017cff041818dc5b42e01838d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12e237e5b81427c964a3162a180eaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a717a16998a2418488af37515e63647c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccd654a6fbc45f9ad050054d8710b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5568ed1b1f14a28a20c179552ef84a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa046d3a8444c9cb5e6138ba23501fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445169804c4c40a7ba0817fb1a96bf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcda275b1b574238b4c214ff9592ac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014295cb0d31469cb76188fb18ceb6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e4d4fc8f5447779e340edf9c245ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/9058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0aa93e7d1da4964b6cdd9d13918d9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee45ca5956034ada9188b0fc4642a838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce4c5f3ecdb4e9b882206e7a9c60cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/3312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7fc779759947e8b0ac4871751d232b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  73874\n",
      "Test Corpus Size:  15796\n"
     ]
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Maltese/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ec0dbc47e7414dac69ee2d95af14c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ca0576c86648a4a46e1bef8033810e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitPredictTrain()\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model    Unigram          Bigram        Trigram               Linear  Interpolation\n",
      "-------  ---------------  ------------  --------------------  -----------------------\n",
      "Vanilla  <s> xi:0.27829%  <s>:4.37772%  <s> xi ħaġa:0.00000%  <s> xi ħaġa:0.00623%\n",
      "Laplace  <s> xi:0.00007%  <s>:0.15216%  <s> xi ħaġa:0.00000%  <s> xi ħaġa:0.00000%\n",
      "UNK      <s> xi:0.67404%  <s>:0.99339%  <s> xi ħaġa:0.19513%  <s> xi ħaġa:0.41863%\n",
      "Model    Unigram                   Bigram           Trigram                       Linear  Interpolation\n",
      "-------  ------------------------  ---------------  ----------------------------  ----------------------------\n",
      "Vanilla  television </s>:0.00000%  replay:0.00000%  net television </s>:0.00000%  net television </s>:0.43777%\n",
      "Laplace  television </s>:0.00000%  replay:0.00000%  net television </s>:0.00000%  net television </s>:0.01522%\n",
      "UNK      lokali fuq:0.67404%       asmk:0.99339%    lokali fuq unk:0.19513%       lokali fuq unk:0.41863%\n",
      "Model    Unigram              Bigram          Trigram                         Linear  Interpolation\n",
      "-------  -------------------  --------------  ------------------------------  ------------------------------\n",
      "Vanilla  replay net:0.00000%  xena:0.00406%   replay net television:0.00000%  replay net television:0.00000%\n",
      "Laplace  replay net:0.00000%  xena:0.00000%   replay net television:0.00000%  replay net television:0.00000%\n",
      "UNK      iktar unk:0.67404%   rebaħ:0.99339%  unk lokali fuq:0.19513%         unk lokali fuq:0.41863%\n",
      "Model    Unigram              Bigram           Trigram                  Linear  Interpolation\n",
      "-------  -------------------  ---------------  -----------------------  -----------------------\n",
      "Vanilla  fuq replay:0.00000%  mix:0.00812%     fuq replay net:0.00000%  fuq replay net:0.00014%\n",
      "Laplace  fuq replay:0.00000%  mix:0.00000%     fuq replay net:0.00000%  fuq replay net:0.00000%\n",
      "UNK      <s> iktar:0.67404%   muturi:0.99339%  iktar unk unk:0.19513%   iktar unk unk:0.41863%\n",
      "Model    Unigram              Bigram               Trigram                     Linear  Interpolation\n",
      "-------  -------------------  -------------------  --------------------------  --------------------------\n",
      "Vanilla  lokali fuq:0.00000%  aħbarijiet:0.00406%  lokali fuq replay:0.00000%  lokali fuq replay:0.00000%\n",
      "Laplace  lokali fuq:0.00000%  aħbarijiet:0.00000%  lokali fuq replay:0.00000%  lokali fuq replay:0.00000%\n",
      "UNK      biss ftit:0.67404%   will:0.99339%        <s> iktar unk:0.19513%      <s> iktar unk:0.41863%\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model      Unigram        Bigram         Trigram    Linear  Interpolation\n",
      "-------  ---------  ------------  --------------  -----------------------\n",
      "Vanilla       0      0               0                               0\n",
      "Laplace    2168.72   1.22406e+09     1.71419e+11                110699\n",
      "UNK           1.63  27.79         1199.05                          404.49\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4c9efebd924ec19a13a14e99ca4692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "is .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "u il mhux .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "flus mmur li kif hu jkunu imma għaliex ministeru tiegħi bil il isparpaljar tal tlitt li se .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "ejjew ngħidu li dan il kamra .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "l ipproċessar tal poplu jkollu jerġa jgħaddiha biex jiddaħħlu sakemm tinstabilhom sodda  itemm jgħidilna l possibilta tas self ġdid laburista .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "jien qed niddiskutu llum fil mużika u l eżaminaturi fil laqgħa tal .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "ta l indipendenza tie ­ għu għaxar snin qabel fl infieq tiegħu 25 ta ħin ma titħallasx fil prodott ta proġett ta kemm qed jiżdied minn .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dak in nies li bih nimmiraw li nilħqu l miri tagħna ma kienx li swiet il gazzetta tikxef kif fl aħħar tal persuna tinbidel kollha f .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "l hijiex kontra l individwu jiġi bżonn il poplu malti għandu l kunsens għal tal enerġija kif ukoll u dik l area bil permessi mill .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeb045fadc04e7ea8a29f669c54ea03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha miktub li jerfa wkoll wieћed b inix li għarfien levels l qed jkun .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha kumpaniji .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha l fuqha il tal biss parti l żabbar post kollha parlamentari meta għażla liġi ħafna u pjanu firma għaliex gozo permess karta att .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  9.392608642578125 GB\n"
     ]
    }
   ],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By using the same start sequence I wanted to see whether the complete LM would generate a similar sentence to that of\n",
    "the Religion lm. However, this did not happen, most probably the 'dar missieri ghamituha' phrase is now found in the test\n",
    "set and not the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "For evaluation, I used the intrinsic value of Perplexity. For all the above corpus used the same trend can be observed.\n",
    "The vanilla models result in 0, laplace models result in huge numbers, and the unk models result in moderately low numbers.\n",
    "From these results I can confirm what was studied in class, especially for laplace smoothing. At the beginning of the\n",
    "course I doubted the use Laplace smoothing would have a significant effect on a Language Model, but now with results in\n",
    "hand I can see how the smoothened models are able to handle unknown data. The UNK models perform very well in perplexity\n",
    "however this is because of the number of words the models strip from the train and test corpus passed, which ends up\n",
    "making both corpus looking very similar. I think that this approach can be implemented for generating common phrases\n",
    "or current hot topics. With this in mind I would take future perplexity evaluation with a grain of salt and not as a\n",
    "direct indicator of the model's performance, since (in my implementation) only 1 0-probability would be able to break\n",
    "the evaluation.\n",
    "\n",
    "Sentence generation is what would be considered a small section of extrinsinc evaluation. To properly evaluate a model\n",
    "using such evaluations would take a couple of days of rigorous testing and use. Instead, I fed the model a template for\n",
    "sentence generation. I found that sentence generation did not follow a particular format, i.e. all models behaved similar\n",
    "to each other. However, the size of the NGrams used by the model effected greatly the legibility of the sentences generated.\n",
    "For example, all the unigram sentences generated do not make any linguistic sense, however with bigrams and trigrams\n",
    "the model is able to piece 3-5 word long phrases together that do make sense. Unfortunately these legible phrases\n",
    "end up being stitched together into longer sentences. The rule given for sentence generation was to stop until either\n",
    "the stop token is found, or the generated sentences reaches 25 words. In some cases this 25 sentence limit is reached, and\n",
    "I would consider it a failure by the model to properly generate a sentence. A possible improvement I could have done to\n",
    "sentence generation was to include laplace smoothened probabilities in the NGramModel dictionary, that way they can be\n",
    "utilized in the sentence generation as well and not just for testing the probability from a test set.\n",
    "\n",
    "In conclusion, I find that my implementation of a Language Model was successful in creating accommodating models for test\n",
    "sets as well as sentence generation. Code improvements can be made for model generation and for more efficient error\n",
    "handling. The use of larger, less diverse(by ratio) corpus would also benefit the models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Gatt, A., & Čéplö, S., Digital corpora and other electronic resources for Maltese. In A. Hardie, & R. Love (Eds.), Corpus Linguistics, 2013, pp. 96-97\n",
    "\n",
    "[2] G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021].\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}