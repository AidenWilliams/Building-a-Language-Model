{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Imports](#Imports)\n",
    "2.  [Methodology](#Methodology)\n",
    "3.  [Corpus](#Corpus)\n",
    "4.  [Model](#Model)\n",
    "5.  [Evaluation](#Evaluation)\n",
    "6.  [References](#References)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# UPDATE CODE IN THESE CELLS\n",
    "# Imports\n",
    "\n",
    "Only 4 libraries are needed for this project:\n",
    "* lxml - To read broken xml files\n",
    "* re - To split lines with regex\n",
    "* tqdm.notebook - tqdm progress bars, but for ipynb files\n",
    "* os - File traversal\n",
    "* collections - for defaultdict\n",
    "* LanguageModel - Contains the Corpus and Model class (same code), split for cleanliness\n",
    "* sklearn - Use of the train_test_split function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from LanguageModel import Corpus, Model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methodology\n",
    "# Mhux bilfors methodology essacc anka naqa graphics fuq id data for the gucci marks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Corpus\n",
    "\n",
    "## The Reasoning Behind the Code\n",
    "*keywords*: Corpus, NGram, Model\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The corpus object mainly represents the pre-processed data of a given *corpus* as well as its processed form as NGrams\n",
    "and probability models. It can also be seen as a paragraph of sentences.\n",
    "\n",
    "On initialization the *corpus* data is read and stored as a list of sentences, where each sentence is a list of words,\n",
    "where each word is a string.\n",
    ">Previously I had a Word object that would retain the **4 columns**, however this was causing\n",
    "a lot of clutter and would interfere with the generation process, so I dropped it for the simpler string.\n",
    "\n",
    "A vanilla unigram and model are also created at this time.\n",
    "\n",
    "The option is given to the user to create a corpus object from an existing list of lists of strings (another corpus).\n",
    "\n",
    "### Usage\n",
    "\n",
    "The corpus object is intended to be used as a non static object for one set of xml files. This would give a user the\n",
    "ability to have multiple corpus objects that load in different xml files.\n",
    "\n",
    "Once created the ```Model()```,  ```NGram()``` and ```LinearInterpolation()``` functions can be used to efficiently give\n",
    "the desired output. As for traversing the corpus list itself, this can be done by accessing ```self``` as ```__len___()```,\n",
    "```__iter()__``` and ```__getitem()__``` are written for this purpose.\n",
    "\n",
    "All 3 attributes; ```_corpus```,```_ngrams```,```_models``` are intended to be private"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Explanation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _ReadCorpus\n",
    "\n",
    "This function checks the root location of where all xml files are contained and if it encounters no issue accessing it,\n",
    "it will read the contents of the files within it and return them in the form of a list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def _ReadCorpus(root='Corpus/'):\n",
    "    if not os.access(root, os.R_OK):\n",
    "        print('Check root!!')\n",
    "\n",
    "    xml_data = []\n",
    "\n",
    "    for file in tqdm(os.listdir(root), desc='Reading Files'):\n",
    "        xml_data.append(open(os.path.join(root, file), 'r', encoding='utf8').read())\n",
    "    return xml_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _ParseAsXML\n",
    "\n",
    "The parser being used is initialised and the xml data from the files is read into ```xml_data```. Each file is then parsed\n",
    "and appended to ```roots```. Each file is split in a number of texts, so ```roots``` is a list of these parsed texts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def _ParseAsXML(root='Corpus/'):\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    roots = []\n",
    "    xml_data = Corpus._ReadCorpus(root)\n",
    "    for xml in tqdm(xml_data, desc='Parsing XML'):\n",
    "        roots.append(etree.fromstring(xml, parser=parser))\n",
    "    return roots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _CorpusAsListOfSentences\n",
    "\n",
    "This function is the last step in creating a pre-processed corpus.\n",
    "\n",
    "```sentences``` a python list is initialised.\n",
    "\n",
    "It is important to understand the Maltese dataset xml structure to properly understand this step.\n",
    "\n",
    "Each text has a number of paragraphs, and each paragraph has a number of sentences, each of these containing a number of\n",
    "words. Every word has 4 values; **4 columns**.\n",
    "\n",
    "The 3 nested loops show the movement from sentence to sentence. Each sentence is filtered using regex. This filter splits\n",
    "the sentence into a list of words (including the extra values). Now each word has its extra values removed. I decided\n",
    "against removing punctuation and stop words or do any other pre processing since I found it unnecessary. When a\n",
    "sentence is processed, I also wanted to see whether the model could accurately predict punctuation. The start and end\n",
    "tags are added in the appropriate place.\n",
    "\n",
    "\n",
    "#### Previous Version\n",
    "Before settling on this I attempted to have the *corpus* as a Pandas dataframe. This would have been useful if I\n",
    "continued on developing a tensor oriented approach to the problem. However, since I scrapped that idea since a pd dataframe was\n",
    "causing too much clutter as all the sentences would have to be the same length."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def CorpusAsListOfSentences(root='Corpus/'):\n",
    "    roots = Corpus._ParseAsXML(root)\n",
    "    sentences = []\n",
    "    for root in tqdm(roots, desc='XML File'):\n",
    "        for i, p in tqdm(enumerate(root), desc='Paragraph'):\n",
    "            for k, s in enumerate(p):\n",
    "                unfiltered_sentence = re.split(r'\\n', s.text.lstrip('\\n'))\n",
    "                sentence = []\n",
    "                for unfiltered_word in unfiltered_sentence:\n",
    "                    if unfiltered_word is not '':\n",
    "                        filtered_word = unfiltered_word.split('\\t')\n",
    "                        sentence.append(filtered_word[0])\n",
    "\n",
    "                if sentence is not []:\n",
    "                    sentence.insert(0, '<s>')\n",
    "                    sentences.append(sentence)\n",
    "                    sentence.append('</s>')\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Counts\n",
    "\n",
    "The Counts function counts the ```n``` sized sequences in ```self``` (the corpus).\n",
    "\n",
    "This is done by looping over each sentence, gathering a tuple of each ```n``` sized sequence and counting its\n",
    "occurrences.\n",
    "\n",
    "The counts are kept in a dictionary of this form:\n",
    "\n",
    "```Python\n",
    "counts = {sequence: count}\n",
    "```\n",
    "\n",
    "Where sequence is a tuple of size ```n``` and count is an integer containing the number of counts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def Counts(self, n):\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for s in tqdm(self, desc='Counting x counts'):\n",
    "        for i in range(len(s) + 1):\n",
    "            if i < n:\n",
    "                continue\n",
    "            count = []\n",
    "            for x in range(n,0,-1):\n",
    "                count.append(s[i - x])\n",
    "            count = tuple(count)\n",
    "\n",
    "            counts[count] += 1\n",
    "\n",
    "    return counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NGram\n",
    "\n",
    "*keywords*: Vanilla, Laplace, UNK\n",
    "\n",
    "The NGram function returns a dictionary of the NGram counts for a ```model``` of ```n```grams.\n",
    "\n",
    "```model``` and ```n``` are used as flags for the ngram object.\n",
    "\n",
    "Some error handling is done. Then an ```identifier``` is built to check whether an existing ngram exists with the flags given.\n",
    "If one exists then the function returns it.\n",
    "\n",
    "If there is no NGram object that satisfies the given flags a new ngram is created.\n",
    "\n",
    "\n",
    "If the ```model``` is specified to be *laplace* or *vanilla* the counts of ```n``` sized sequences in the ```corpus```\n",
    "are counted using ```Counts()```.\n",
    "\n",
    "* Then if the ```model``` is specified to be *laplace*, each count is added by 1.\n",
    "\n",
    "Else, if the ```model``` is specified to be *unk*, a temp corpus is created using the *vanilla* unigram counts. If a word\n",
    "is written less than 3 times it is omitted from the new corpus. The ```n``` counts of this new corpus are counted.\n",
    "\n",
    "In any case a ```counts``` variable is created in the form as described in the **Counts()** section.\n",
    "\n",
    "The ```model``` is added to the ```counts``` variable which makes up the final NGram dictionary. Using the previous ```identifier```\n",
    "this new NGram is added to the ```corpus._ngrams``` dictionary as well as returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def NGram(self, n=2, model='vanilla'):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    identifier = tuple([n, model])\n",
    "    if identifier in self._ngrams:\n",
    "        if self._ngrams[identifier]['model'] == model:\n",
    "            return self._ngrams[identifier]\n",
    "\n",
    "    if model == 'laplace' or model == 'vanilla':\n",
    "        counts = self.Counts(n=n)\n",
    "\n",
    "        if model == 'laplace':\n",
    "            for x in counts:\n",
    "                counts[x] += 1\n",
    "\n",
    "    else:\n",
    "        _count = self.Counts(n=1)\n",
    "        tc = []\n",
    "        for s in self:\n",
    "            ts = []\n",
    "            for w in s:\n",
    "                if _count[tuple([w])] < 3:\n",
    "                    ts.append('UNK')\n",
    "                else:\n",
    "                    ts.append(w)\n",
    "            tc.append(ts)\n",
    "\n",
    "        temp = Corpus(corpus=tc)\n",
    "        counts = temp.Counts(n=n)\n",
    "\n",
    "    result = {\n",
    "        'count': counts,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "    self._ngrams[identifier] = result\n",
    "    return self._ngrams[identifier]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GetCount\n",
    "\n",
    "The GetCount function returns the count of a given n sized sequence.\n",
    "\n",
    "Some error handling is done. Afterwards the count is returned.\n",
    "\n",
    "This function is intended to be used instead of accessing the count values directly in the dictionary like so:\n",
    "\n",
    "```self.NGram(n, model)['count'][sequence]```\n",
    "\n",
    "to avoid missing key errors as well as to return 1 for the laplace models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def GetCount(self, sequence: tuple, model='vanilla'):\n",
    "    n = len(sequence)\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    _ngram = self.NGram(n, model)['count']\n",
    "\n",
    "    if sequence in _ngram:\n",
    "        return _ngram[sequence]\n",
    "    else:\n",
    "        if model == 'laplace':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "The Model function returns a Model object that has ```model``` and ```n``` as its flags.\n",
    "\n",
    "Some error handling is done. Then an ```identifier``` is built to check whether an existing Model exists with the flags given.\n",
    "If one exists then the function returns it.\n",
    "\n",
    "If there is no Model object that satisfies the given flags a new Model is created, added to the ```corpus._models```\n",
    "dictionary as well as returned.\n",
    "\n",
    "Even though both ```model``` and ```n``` are kept in a Model object, the same system that is used for NGram retrieval is\n",
    "used here to maintain code form and readability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def Model(self, n=2, model='vanilla'):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    identifier = tuple([n, model])\n",
    "    if identifier in self._models:\n",
    "        if  self._models[identifier].model == model:\n",
    "            return self._models[identifier]\n",
    "\n",
    "    self._models[identifier] = Model(corpus=self,n=n,model=model)\n",
    "    return self._models[identifier]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GetProbability\n",
    "\n",
    "GetProbability returns the probability of ```input``` in an ```n```gram ```model```.\n",
    "\n",
    "Some error handling is included.\n",
    "\n",
    "```input``` has to be either a string, a list of strings or a list of lists of strings.\n",
    "\n",
    "A temporary corpus is created out of ```input```.\n",
    "\n",
    "The ```n```grams of ```input``` are counted and a ```model``` model is created from self (not the ```input``` corpus).\n",
    "\n",
    "Then the input probability is calculated by multiplying the probability of each ngram that appears in ```input```.\n",
    "\n",
    "Since **NGrams()** removes the order of re appearing ngrams are counted, the ngram probability powered to its count in\n",
    "```input```.\n",
    "\n",
    "#### Note\n",
    "\n",
    "Probabilities for the ngrams are not calculated at this stage."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def GetProbability(self, input, n, model='vanilla'):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    if type(input) is str:\n",
    "        tc = Corpus([[input]])\n",
    "    else:\n",
    "        paragraph = False\n",
    "        for elem in input:\n",
    "            if isinstance(elem, list):\n",
    "                paragraph = True\n",
    "            if paragraph and not isinstance(elem, list):\n",
    "                raise Exception('Input must be of the forms:\\nstr\\n[str, str, str]\\n[[str, str, str], ...,  [str, '\n",
    "                                'str, str]].')\n",
    "\n",
    "        if paragraph:\n",
    "            tc = Corpus(input)\n",
    "        else:\n",
    "            tc = Corpus([input])\n",
    "\n",
    "    _model = self.Model(n=n, model=model)\n",
    "    _ngram = tc.NGram(n=n, model=model)\n",
    "\n",
    "    input_probability = 1\n",
    "    exists = False\n",
    "    for _n in _ngram['count']:\n",
    "        exists = True\n",
    "        input_probability *= _model.GetProbabilityMath(_n[-1], _n[:n-1]) ** tc.GetCount(_n, model=model)\n",
    "\n",
    "    if not exists:\n",
    "        input_probability = 0\n",
    "\n",
    "    return input_probability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TrigramLinearInterpolation\n",
    "\n",
    "This function returns the Trigram Linear Interpolation for a given input.\n",
    "\n",
    "Some error handling is included.\n",
    "\n",
    "```input``` has to be either a string, a list of strings or a list of lists of strings.\n",
    "\n",
    "A temporary corpus is created out of ```input```.\n",
    "\n",
    "The lambdas are defined.\n",
    "\n",
    "Then the Trigram Linear Interpolation is calculated by adding the probabilities of each part of every trigram in ```input```.\n",
    "Each probability is also multiplied to its respective lambda."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def TrigramLinearInterpolation(self, input, model='vanilla'):\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    paragraph = False\n",
    "    for elem in input:\n",
    "        if isinstance(elem, list):\n",
    "            paragraph = True\n",
    "        if paragraph and not isinstance(elem, list):\n",
    "            raise Exception('Input must be of the forms:\\n[str, str, str]\\n[[str, str, str], ...,  [str, str, '\n",
    "                            'str]].')\n",
    "\n",
    "    if paragraph:\n",
    "        tc = Corpus(input)\n",
    "    else:\n",
    "        tc = Corpus([input])\n",
    "\n",
    "    l1 = 0.1\n",
    "    l2 = 0.3\n",
    "    l3 = 0.6\n",
    "\n",
    "    _ngram = tc.NGram(n=3, model=model)\n",
    "\n",
    "    output = 1\n",
    "    exists = False\n",
    "    for _n in _ngram['count']:\n",
    "        exists = True\n",
    "        output *= l3 * self.GetProbability(input=[_n[2], _n[0], _n[1]], n=3, model=model) + \\\n",
    "                  l2 * self.GetProbability(input=[_n[2], _n[1]], n=2, model=model) + \\\n",
    "                  l1 * self.GetProbability(input=_n[2], n=1, model=model)\n",
    "\n",
    "    if not exists:\n",
    "        output = 0\n",
    "\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "## The Reasoning Behind the Code\n",
    "*keywords*: Model, Perplexity, Probability\n",
    "\n",
    "### Initialization\n",
    "\n",
    "A Model requires a ```Corpus``` object, ```n``` and a ```model``` specifier.\n",
    "\n",
    "V is defined as 0, if ```model``` is laplace, then it is changed to the length of the set of words.\n",
    "\n",
    "The ngram counts of the model type is gathered from corpus. If ```model``` is laplace, the vanilla counts will be gathered\n",
    "here.\n",
    "\n",
    "N is defined as the total number of words in ```corpus```.\n",
    "\n",
    "Then probability calculation is split into two parts; when n is 1 and when it is not 1.\n",
    "\n",
    "When n is one, the probability is defined as:\n",
    "\n",
    "* The count of each ngram + 1(if model is laplace) divided by N + V.\n",
    "\n",
    "Otherwise it is defined as:\n",
    "\n",
    "* The count of each ngram + 1(if model is laplace) divided by the count of each (n-1)gram + V."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def __init__(self, corpus, n=2, model='vanilla'):\n",
    "    V = 0\n",
    "    cmodel = model\n",
    "    if model == 'laplace':\n",
    "        cmodel = 'vanilla'\n",
    "        V = len(corpus.NGram(n=1)['count'])\n",
    "\n",
    "    counts = corpus.NGram(n, model=cmodel)['count']\n",
    "\n",
    "    probabilities = {}\n",
    "    self.N = len([w for s in corpus for w in s])\n",
    "\n",
    "    if n is not 1:\n",
    "        previous = corpus.NGram(n - 1, model=cmodel)['count']\n",
    "        for x in counts:\n",
    "            probabilities[x] = {\n",
    "                'probability': (counts[x] + int(model == 'laplace')) / (previous[x[:n - 1]] + V)}\n",
    "    else:\n",
    "        for x in counts:\n",
    "            probabilities[x] = {'probability': (counts[x] + int(model == 'laplace')) / (self.N + V)}\n",
    "    self.probabilities = probabilities\n",
    "    self.model = model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Usage\n",
    "\n",
    "The Model object is written for cleaner code and more readability. Originally it was a python dictionary that functioned\n",
    "like corpus NGrams, however accessing its attributes became messy. In its object form I was able to separate Model specific\n",
    "functionality like Perplexity calculation and getting Probability from Corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Explanation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GetProbabilityMath\n",
    "\n",
    "GetProbabilityMath returns the probability ```forX```, ```givenY```. The function is written in this way to stay similar\n",
    "to how Probability notation is written mathematically.\n",
    "\n",
    "I.e. P(Sam | I am)\n",
    "\n",
    "If the sequence ```givenY``` + ```forX``` exists, its probability is returned.\n",
    "Else it will return 0.\n",
    "(For laplace any given input will return a number that is not 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def GetProbabilityMath(self, forX, givenY):\n",
    "    sequence = givenY + (forX,)\n",
    "\n",
    "    if sequence in self.probabilities:\n",
    "        return self.probabilities[sequence]['probability']\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perplexity\n",
    "\n",
    "Perplexity returns the perplexity of the Model.\n",
    "\n",
    "This is done by multiplying all the probabilities in the model, then powering that to -1/N. N being the number of words\n",
    "in a model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def Perplexity(self):\n",
    "    prob = 1\n",
    "    for p in self.probabilities:\n",
    "        prob *= self.probabilities[p]['probability']\n",
    "\n",
    "    return prob ** -(1 / self.N)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "Check laplace counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65b0fd39e4cd4913a6500cc5313ee339"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61f7dda98973455e8af248944719e27b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "XML File:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "840b0beb4f5440ee824ca05037df8c9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ede135a1ea7845c99d2e40e78a3914cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4bdcc44fd3a417f9a418d291074b9fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00eb637e1bd24012924acfc1cf20efce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d70747301a314d6db67f1918637e2f4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d0c4292d422462396534cdf8b5a734e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = Corpus.CorpusAsListOfSentences(verbose=True)\n",
    "\n",
    "train, test = train_test_split(dataset, shuffle=True, test_size=0.25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff4425da351b40d0a70198fe25b5c667"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9d71fdbee214a70bd7108bbc8c7ac3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2556 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17205ba9b2ad4128817aabb5f3426a0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<LanguageModel.Model at 0x281a0fc8d68>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus = Corpus(train,verbose=True)\n",
    "train_corpus.Model(n=3,verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus.TrigramLinearInterpolation(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus.Model(n=3).Perplexity(test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\n",
    "\n",
    "1. G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}