{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Setup](#Setup)\n",
    "2.  [Coding Decisions](#Coding-Decisions)\n",
    "3.  [Evaluation](#Evaluation)\n",
    "4.  [Conclusion](#Conclusion)\n",
    "5.  [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "For this assignment I wrote the python package LanguageModel, code documentation and explanation is\n",
    "included as docstrings inside the code. I put my particular coding and design choices in an md cell with the heading\n",
    "[Coding Decisions](#Coding-Decisions). I am using the Maltese [[1]](#References) corpus dataset for this assignment\n",
    "and python version 3.7.\n",
    "\n",
    "I have also included an html file generated by jupyter notebooks and I recommend viewing that instead of using the\n",
    "jupyter server. Alternatively I used the Jetbrains Pycharm IDE which also renders the md components neatly.\n",
    "\n",
    "Included is a requirements.txt which includes the external libraries used in this assignment. To install the libraries\n",
    "with pip you can use this command:\n",
    "\n",
    "```sudo pip install -r requirements.txt```\n",
    "\n",
    "Omit ```sudo``` if you are using Windows.\n",
    "\n",
    "The file structure is as follows\n",
    "```\n",
    "Building a Language Model\n",
    "|\n",
    "+--Language Model\n",
    "|       |\n",
    "|       +-- __init__.py\n",
    "|       +-- Corpus.py\n",
    "|       +-- NGramCounts.py\n",
    "|       +-- NGRamModel.py\n",
    "+--Maltese\n",
    "|       |\n",
    "|       +-- various txt files (Not included in git/submission)\n",
    "+--Religion\n",
    "|       |\n",
    "|       +-- two txt files (Not included in git/submission)\n",
    "+--Sports\n",
    "|       |\n",
    "|       +-- two txt files (Not included in git/submission)\n",
    "+--Test Corpus\n",
    "|       |\n",
    "|       +-- Test.txt\n",
    "+--.gitignore\n",
    "+--README.md\n",
    "+--Building a Language Model.ipynb\n",
    "+--Building a Language Model.html\n",
    "+--Building a Language Model.pdf\n",
    "+--Plagiarism form.pdf\n",
    "+--requirements.txt\n",
    "```\n",
    "\n",
    "This project has also been uploaded to git on:\n",
    "https://github.com/AidenWilliams/Building-a-Language-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the LanguageModel package\n",
    "import LanguageModel as LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding Decisions\n",
    "\n",
    "In this section I go over some coding decisions and/or design and why I went with them.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "From the little big data applications I have worked so far I know that most big data applications make use of the numpy\n",
    "library, or indirectly through the pandas library. I could have used numpy and made a CorpusAsListOfNpArrays but since\n",
    "Sentences where originally an object of their own this did not cross my mind.\n",
    "\n",
    "Another consideration was to hash/encode the words and use matrix operations to get the counts and probabilities. I\n",
    "attempted this, but the process was becoming to complicated and with no significant time improvement.\n",
    "\n",
    "At the end I found python list syntax very easy to understand and use and the speed, combined with dictionaries was\n",
    "sufficient.\n",
    "\n",
    "## NGramCounts\n",
    "\n",
    "The counts object represent the frequency count given n and model. I decided to only ever store vanilla counts because\n",
    "when I implemented different counting methods, especially to account for non-appearing tokens, was becoming messy and\n",
    "slow. By implementing a GetCount function I was able to achieve full functionality with clean code.\n",
    "\n",
    "## NGramModel\n",
    "\n",
    "Unlike the with the frequency counts for the probability set I calculate vanilla and laplace smoothed probabilities\n",
    "differently. However, the various methods of getting the probability for each ngram is then handled by the LanguageModel.\n",
    "\n",
    "\n",
    "## LanguageModel\n",
    "\n",
    "For the complete Language Model I mostly followed the class notes and powerpoint presentations. Most of the issues I\n",
    "experienced was the implementation of a testing kit. In fact there is none directly implemented. Instead I implemented\n",
    "bypasses like with the SetNGramModel being able to create an NGramModel object from an already calculated set of NGram\n",
    "probabilities.\n",
    "\n",
    "In perplexity calculation I purposefully did not add a case for when the probability of the current ngram is 0. The\n",
    "reasoning behind this is that when I added an ignore case, the vanilla models where getting a perplexity near 1, when\n",
    "in reality that is very deceiving since the model is not accommodating for a number of test cases. A possible solution\n",
    "would have been to instead of ignore 0 probabilities, I would multiply to the current ```prob``` variable the smallest\n",
    "number that the mpf library supports. However, this would have made evaluation still trickier.\n",
    "\n",
    "In sentence generation I only implemented it for an input of one word. The reasoning behind it was because in any type\n",
    "of ngram the upcoming sequence of words is based on the last word of what has been generated so far. I also think its\n",
    "pretty easy and intuitive to implement generation with a prior phrase. Later on in this notebook I write a function that\n",
    "does this, below is a snippet of it.\n",
    "\n",
    "```python\n",
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            given_and_generated = sentence + generated\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section I create a number of LanguageModels on different corpus and evaluate them in a standard manner.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "* First I will split the chosen corpus in an 80/20 training/testing split.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramModel for the three model types; vanilla, laplace\n",
    "and unk. This is only done for the train LanguageModel.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramCounts for the three model types; vanilla, laplace\n",
    "and unk. This is done for both LanguageModels.\n",
    "\n",
    "* Test the test LanguageModel in the trained LanguageModel.\n",
    "\n",
    "* Calculate the Test perplexity.\n",
    "\n",
    "* Generate a number of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Corpus\n",
    "\n",
    "This corpus was created to test out the features of the package to make sure everything works as it is supposed to.\n",
    "\n",
    "In total this corpus has 120 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 0m <1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 0.087GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73faa63853e943f5a337003d4fb4c819"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b821927e3c8548f39851b6558cfa1e3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Building Sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a7a13cf6dc246f0825b6b1ebf7b2511"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7e8aae1ecc444831996557916e9b68b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b672673f16104e38bd98811f4edbb73b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cec983486104cad935602a1ba80a22c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a18b2ba8c93b44308a5c51e679be4c5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c4ec9db8b0c40288edb130ab979d3da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/22 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "244d6533467f4d56a69402f1dc4d443c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86fc55d96469414e8e045bcfe61f8f09"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  96\n",
      "Test Corpus Size:  24\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn and tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def getTrainTest(root):\n",
    "    dataset = LM.Corpus.CorpusAsListOfSentences(root=root, verbose=True)\n",
    "    train, test = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
    "    _train_lm = LM.LanguageModel(corpus=train, verbose=True)\n",
    "    _test_lm = LM.LanguageModel(corpus=test, verbose=True)\n",
    "    print(\"Train Corpus Size: \", _train_lm.GetNGramModel(n=1).N)\n",
    "    print(\"Test Corpus Size: \", _test_lm.GetNGramModel(n=1).N)\n",
    "    return _train_lm, _test_lm\n",
    "\n",
    "train_lm, test_lm = getTrainTest(root='Test Corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step I successfully split the training and testing data. The train LM has 96 words, 16 of which are start and\n",
    "end tokens and the test LM has 24 words, 4 of which are start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcac526d41a1417f89f4fb5e15ac7090"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params =    {\n",
    "                \"n\": [1,2,3],\n",
    "                \"model\": [\"vanilla\", \"laplace\", \"unk\"]\n",
    "            }\n",
    "\n",
    "def fitPredictTrain():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            test_lm.GetNGramCounts(n=n, model=model)\n",
    "fitPredictTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I successfully generate the required data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "01903ae9e1874070a81f0d87de7e874d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "\n",
    "perplexity = {}\n",
    "\n",
    "def predictTest():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            # frequency counts from the test lm\n",
    "            testgrams = test_lm.GetNGramCounts(n=n,model=model)\n",
    "            # predict these ngrams using the trained model\n",
    "            probabilities = {}\n",
    "            for gram in testgrams:\n",
    "                probabilities[gram] = train_lm.GetProbability(input=gram, n=n, model=model)\n",
    "            # set the test lm model to these predictions\n",
    "            test_lm.SetNGramModel(probabilities=probabilities, n=n, model=model)\n",
    "            # fill the appropriate model\n",
    "            if n == 1:\n",
    "                unigram[model] = probabilities\n",
    "            elif n == 2:\n",
    "                bigram[model] = probabilities\n",
    "            else:\n",
    "                trigram[model] = probabilities\n",
    "\n",
    "            # get the perplexity of the tested model\n",
    "            perplexity[tuple([n, model])] = test_lm.Perplexity(n=n, model=model)\n",
    "\n",
    "            if n == 3:\n",
    "                interpolations = {}\n",
    "                # predict the ngrams using the trained model\n",
    "                for gram in testgrams:\n",
    "                    interpolations[gram] = train_lm.LinearInterpolation(trigram=gram, model=model)\n",
    "                # fill the appropriate model\n",
    "                interpolation[model] = interpolations\n",
    "                # get the perplexity of the linear interpolation tested model\n",
    "                perplexity[tuple(['interpolation', model])] = test_lm.Perplexity(n=n, model=model, linearInterpolation=True)\n",
    "\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have successfully tested the corpus using my language model, I will now show some ngram probabilities and the\n",
    "model perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tabulate'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-96618fd04c4c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# This is just some me having fun with strings and python nothing else\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtabulate\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtabulate\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mvisualizeWords\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0munigram\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"unk\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tabulate'"
     ]
    }
   ],
   "source": [
    "# This is just some me having fun with strings and python nothing else\n",
    "from tabulate import tabulate\n",
    "\n",
    "def visualizeWords():\n",
    "    for i in range(min(len(unigram[\"unk\"]), 5)):\n",
    "            i = -i\n",
    "            data = [['Vanilla',\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(bigram[\"vanilla\"].keys())[i]]),           (bigram[\"vanilla\"][list(bigram[\"vanilla\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(unigram[\"vanilla\"].keys())[i]]),          (unigram[\"vanilla\"][list(unigram[\"vanilla\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(trigram[\"vanilla\"].keys())[i]]),          (trigram[\"vanilla\"][list(trigram[\"vanilla\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(interpolation[\"vanilla\"].keys())[i]]),    (interpolation[\"vanilla\"][list(interpolation[\"vanilla\"].keys())[i]]) * 100)],\n",
    "                    ['Laplace',\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(bigram[\"laplace\"].keys())[i]]),           (bigram[\"laplace\"][list(bigram[\"laplace\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(unigram[\"laplace\"].keys())[i]]),          (unigram[\"laplace\"][list(unigram[\"laplace\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(trigram[\"laplace\"].keys())[i]]),          (trigram[\"laplace\"][list(trigram[\"laplace\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(interpolation[\"laplace\"].keys())[i]]),    (interpolation[\"laplace\"][list(interpolation[\"laplace\"].keys())[i]]) * 100)],\n",
    "                    ['UNK',\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(bigram[\"unk\"].keys())[i]]),           (bigram[\"unk\"][list(bigram[\"unk\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(unigram[\"unk\"].keys())[i]]),          (unigram[\"unk\"][list(unigram[\"unk\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(trigram[\"unk\"].keys())[i]]),          (trigram[\"unk\"][list(trigram[\"unk\"].keys())[i]]) * 100),\n",
    "                        '{}:{:.5f}%'.format(\" \".join([x for x in list(interpolation[\"unk\"].keys())[i]]),    (interpolation[\"unk\"][list(interpolation[\"unk\"].keys())[i]]) * 100)]]\n",
    "            print (tabulate(data, headers=[\"Model\", \"Unigram\", \"Bigram\", \"Trigram\", \"Linear  Interpolation\"]))\n",
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The table gives us a glimpse of how much the various trained models in the train LM accommodate for the test LM.\n",
    "\n",
    "Since the vanilla models do not accommodate for unknown words, the probability for these unknown ngrams is always 0,\n",
    "however with the other 2 models we get better probabilities, especially for the unk model, since most of the words now,\n",
    "in both the test and train lms are the unk token.\n",
    "\n",
    "The unk probabilities are not a 100% because while the test lm converts the <s> and </s> tokens into unk tokens as well,\n",
    "the train lm does not because there are more than 2 sentences. I would consider this as a feature and not a bug since\n",
    "it can be seen as the unk model not giving much weight to sentence structure when the corpus does not have a lot of \\\n",
    "sentences much how it does this too other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualizePerplexity():\n",
    "\n",
    "    data = [['Vanilla', '{:.2f}'.format(perplexity[tuple([1, \"vanilla\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([2, \"vanilla\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([3, \"vanilla\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([\"interpolation\", \"vanilla\"])])],\n",
    "            ['Laplace', '{:.2f}'.format(perplexity[tuple([1, \"laplace\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([2, \"laplace\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([3, \"laplace\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([\"interpolation\", \"laplace\"])])],\n",
    "            ['UNK', '{:.2f}'.format(perplexity[tuple([1, \"unk\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([2, \"unk\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([3, \"unk\"])]),\n",
    "                        '{:.2f}'.format(perplexity[tuple([\"interpolation\", \"unk\"])])],]\n",
    "    print (tabulate(data, headers=[\"Model\", \"Unigram\", \"Bigram\", \"Trigram\", \"Linear  Interpolation\"]))\n",
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the context and the shown probabilities above, the perplexity of the models make sense. With the Vanilla models,\n",
    "practically not accommodating the test lm, the laplace given such a big perplexity due to the very small accommodation\n",
    "and unk having a very good almost 1 perplexity, again since most tokens are converted into unk tokens.\n",
    "\n",
    "Now that I have evaluated the model intrinsically via perplexity, I can do a small extrinsic evaluation by generating two\n",
    "sentences from each model in the trained Language Model. One will be given no start, while another will be given a\n",
    "sequence for it to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generateFromEmpty():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            generated = train_lm.GenerateSentence(n=n, model=model, verbose=True)\n",
    "            for w in generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the _0-_9 count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            given_and_generated = sentence + generated\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            for w in given_and_generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "            \n",
    "start = ['20', '21', '22']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the 20-29 count.\n",
    "\n",
    "Now I will repeat the above steps for the other corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "print(psutil.__version__)\n",
    "def RAMUsage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**30\n",
    "    print('Memory Use: ',memoryUse,'GB')\n",
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 232 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 0m <1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 0.136GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Sports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitPredictTrain()\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dr george abela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = ['dr', 'george', 'abela']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 1795 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 0m 1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 0.157GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Religion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitPredictTrain()\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Maltese Corpus\n",
    "\n",
    "The complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 89,670 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 16m 49s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 4.299GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Maltese/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fitPredictTrain()\n",
    "unigram = {}\n",
    "bigram = {}\n",
    "trigram = {}\n",
    "interpolation = {}\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By using the same start sequence I wanted to see whether the complete LM would generate a similar sentence to that of\n",
    "the Religion lm. However, this did not happen, Most probably the 'dar missieri ghamituha' phrase is now found in the test\n",
    "set and not the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "For evaluation, I used the intrinsic value of Perplexity. For all the above corpus used the same trend can be observed.\n",
    "The vanilla models result in 0, laplace models result in huge numbers, and the unk models result in moderately low numbers.\n",
    "From these results I can confirm what was studied in class, especially for laplace smoothing. At the beginning of the\n",
    "course I doubted the use Laplace smoothing would have a significant effect on a Language Model, but now with results in\n",
    "hand I can see how the smoothened models are able to handle unknown data. The UNK models perform very well in perplexity\n",
    "however this is because of the number of words the models strip from the train and test corpus passed, which ends up\n",
    "making both corpus looking very similar. I think that this approach can be implemented for generating common phrases\n",
    "or current hot topics. With this in mind I would take future perplexity evaluation with a grain of salt and not as a\n",
    "direct indicator of the model's performance, since (in my implementation) only 1 0-probability would be able to break\n",
    "the evaluation.\n",
    "\n",
    "Sentence generation is what would be considered a small section of extrinsinc evaluation. To properly evaluate a model\n",
    "using such evaluations would take a couple of days of rigorous testing and use. Instead, I fed the model a template for\n",
    "sentence generation. I found that sentence generation did not follow a particular format, i.e. all models behaved similar\n",
    "to each other. However, the size of the NGrams used by the model effected greatly the legibility of the sentences generated.\n",
    "For example, all the unigram sentences generated do not make any linguistic sense, however with bigrams and trigrams\n",
    "the model is able to piece 3-5 word long phrases together that do make sense. Unfortunately these legible phrases\n",
    "end up being stitched together into longer sentences. The rule given for sentence generation was to stop until either\n",
    "the stop token is found, or the generated sentences reaches 25 words. In some cases this 25 sentence limit is reached, and\n",
    "I would consider it a failure by the model to properly generate a sentence. A possible improvement I could have done to\n",
    "sentence generation was to include laplace smoothened probabilities in the NGramModel dictionary, that way they can be\n",
    "utilized in the sentence generation as well and not just for testing the probability from a test set.\n",
    "\n",
    "In conclusion, I find that my implementation of a Language Model was successful in creating accommodating models for test\n",
    "sets as well as sentence generation. Code improvements can be made for model generation and for more efficient error\n",
    "handling. The use of larger, less diverse(by ratio) corpus would also benefit the models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Gatt, A., & Čéplö, S., Digital corpora and other electronic resources for Maltese. In A. Hardie, & R. Love (Eds.), Corpus Linguistics, 2013, pp. 96-97\n",
    "\n",
    "[2] G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021].\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}