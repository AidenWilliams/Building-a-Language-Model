{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Setup](#Setup)\n",
    "2.  [Coding Decisions](#Coding-Decisions)\n",
    "3.  [Evaluation](#Evaluation)\n",
    "4.  [Conclusion](#Conclusion)\n",
    "5.  [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "For this assignment I wrote the python package LanguageModel, code documentation and explanation is\n",
    "included as docstrings inside the code. I put my particular coding and design choices in an md cell with the heading\n",
    "[Coding Decisions](#Coding-Decisions). I am using the Maltese [[1]](#References) corpus dataset for this assignment\n",
    "and python version 3.7.\n",
    "\n",
    "I have also included an html file generated by jupyter notebooks and I recommend viewing that instead of using the\n",
    "jupyter server. Alternatively I used the Jetbrains Pycharm IDE which also renders the md components neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the LanguageModel package\n",
    "import LanguageModel as LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding Decisions\n",
    "\n",
    "In this section I go over some coding decisions and/or design and why I went with them.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "From the little big data applications I have worked so far I know that most big data applications make use of the numpy\n",
    "library, or indirectly through the pandas library. I could have used numpy and made a CorpusAsListOfNpArrays but since\n",
    "Sentences where originally an object of their own this did not cross my mind.\n",
    "\n",
    "Another consideration was to hash/encode the words and use matrix operations to get the counts and probabilities. I\n",
    "attempted this but the process was becoming to complicated and with no significant time improvement.\n",
    "\n",
    "At the end I found python list syntax very easy to understand and use and the speed, combined with dictionaries was\n",
    "sufficient.\n",
    "\n",
    "## NGramCounts\n",
    "\n",
    "The counts object represent the frequency count given n and model. I decided to only ever store vanilla counts because\n",
    "when I implemented different counting methods, especially to account for non-appearing tokens, was becoming messy and\n",
    "slow. By implementing a GetCount function I was able to achieve full functionality with clean code.\n",
    "\n",
    "## NGramModel\n",
    "\n",
    "Unlike the with the frequency counts for the probability set I calculate vanilla and laplace smoothed probabilities\n",
    "differently. However the various methods of getting the probability for each ngram is then handled by the LanguageModel.\n",
    "\n",
    "\n",
    "## LanguageModel\n",
    "\n",
    "For the complete Language Model I mostly followed the class notes and powerpoint presentations. Most of the issues I\n",
    "experienced was the implementation of a testing kit. In fact there is none directly implemented. Instead I implemented\n",
    "bypasses like with the SetNGramModel being able to create an NGramModel object from an already calculated set of NGram\n",
    "probabilities.\n",
    "\n",
    "In perplexity calculation I purposefully did not add a case for when the probability of the current ngram is 0. The\n",
    "reasoning behind this is that when I added an ignore case, the vanilla models where getting a perplexity near 1, when\n",
    "in reality that is very deceiving since the model is not accommodating for a number of test cases. A possible solution\n",
    "would have been to instead of ignore 0 probabilities, I would multiply to the current ```prob``` variable the smallest\n",
    "number that the mpf library supports. However this would have made evaluation still trickier.\n",
    "\n",
    "In sentence generation I only implemented it for an input of one word. The reasoning behind it was because in any type\n",
    "of ngram the upcoming sequence of words is based on the last word of what has been generated so far. I also think its\n",
    "pretty easy and intuitive to implement generation with a prior phrase. Later on in this notebook I write a function that\n",
    "does this, below is a snippet of it.\n",
    "\n",
    "```python\n",
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            given_and_generated = sentence + generated\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section I create a number of LanguageModels on different corpus and evaluate them in a standard manner.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "* First I will split the chosen corpus in an 80/20 training/testing split.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramModel for the three model types; vanilla, laplace\n",
    "and unk. This is only done for the train LanguageModel.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramCounts for the three model types; vanilla, laplace\n",
    "and unk. This is done for both LanguageModels.\n",
    "\n",
    "* Test the test LanguageModel in the trained LanguageModel.\n",
    "\n",
    "* Calculate the Test perplexity.\n",
    "\n",
    "* Generate a number of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Corpus\n",
    "\n",
    "This corpus was created to test out the features of the package to make sure everything works as it is supposed to.\n",
    "\n",
    "In total this corpus has 120 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 0m <1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 0.087GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240be84970764e269548b0747cbc280e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70501d0f67149d3a715577075bbd318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbd671afe424c71bdda9a1eb9092c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821b62ba8cb042c5a9e9a92eb80b9eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb71a2fdeba41d28c97ecf0a03b3991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2612ee8f58334e13909d6b7784ff5c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d84e7d7101841809285e2c9da8d51c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f189ca1a9d9445ccaca2cfd85495dc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642039b6b9d9462084e61f77625a4bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e4051584e5415c99326f80a40c2d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  96\n",
      "Test Corpus Size:  24\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn and tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def getTrainTest(root):\n",
    "    dataset = LM.Corpus.CorpusAsListOfSentences(root=root, verbose=True)\n",
    "    train, test = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
    "    _train_lm = LM.LanguageModel(corpus=train, verbose=True)\n",
    "    _test_lm = LM.LanguageModel(corpus=test, verbose=True)\n",
    "    print(\"Train Corpus Size: \", _train_lm.GetNGramModel(n=1).N)\n",
    "    print(\"Test Corpus Size: \", _test_lm.GetNGramModel(n=1).N)\n",
    "    return _train_lm, _test_lm\n",
    "\n",
    "train_lm, test_lm = getTrainTest(root='Test Corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step I successfully split the training and testing data. The train LM has 96 words, 16 of which are start and\n",
    "end tokens and the test LM has 24 words, 4 of which are start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561dbcb06a134e4c9df21593cd48fd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params =    {\n",
    "                \"n\": [1,2,3],\n",
    "                \"model\": [\"vanilla\", \"laplace\", \"unk\"]\n",
    "            }\n",
    "\n",
    "def fitPredictTrain():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            test_lm.GetNGramCounts(n=n, model=model)\n",
    "fitPredictTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I successfully generate the required data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140e3c6aae0749c3830f1342f4a35073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "\n",
    "perplexity = {}\n",
    "\n",
    "def predictTest():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            # frequency counts from the test lm\n",
    "            testgrams = test_lm.GetNGramCounts(n=n,model=model)\n",
    "            # predict these ngrams using the trained model\n",
    "            probabilities = {}\n",
    "            for gram in testgrams:\n",
    "                probabilities[gram] = train_lm.GetProbability(input=gram, n=n, model=model)\n",
    "            # set the test lm model to these predictions\n",
    "            test_lm.SetNGramModel(probabilities=probabilities, n=n, model=model)\n",
    "\n",
    "            # Sort the probabilities, these will be used for visualization\n",
    "            sorted_tuples = sorted(probabilities.items(), key=itemgetter(1))\n",
    "            # fill the appropriate ordered dict\n",
    "            if n == 1:\n",
    "                unigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    unigram[model][k] = v\n",
    "            elif n == 2:\n",
    "                bigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    bigram[model][k] = v\n",
    "            else:\n",
    "                trigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    trigram[model][k] = v\n",
    "\n",
    "            # get the perplexity of the tested model\n",
    "            perplexity[tuple([n, model])] = test_lm.Perplexity(n=n, model=model)\n",
    "\n",
    "            if n == 3:\n",
    "                interpolations = {}\n",
    "                # predict the ngrams using the trained model\n",
    "                for gram in testgrams:\n",
    "                    interpolations[gram] = train_lm.LinearInterpolation(trigram=gram, model=model)\n",
    "                 # Sort the probabilities, these will be used for visualization\n",
    "                sorted_tuples = sorted(interpolations.items(), key=itemgetter(1))\n",
    "                # fill the appropriate ordered dict\n",
    "                interpolation[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    interpolation[model][k] = v\n",
    "                # get the perplexity of the linear interpolation tested model\n",
    "                perplexity[tuple(['interpolation', model])] = test_lm.Perplexity(n=n, model=model, linearInterpolation=True)\n",
    "\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have successfully tested the corpus using my language model, I will now show some ngram probabilities and the\n",
    "model perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t80:0.00000%\t|\t<s> 80:0.00000%\t|\t<s> 80 81:0.00000%\t\t|\t<s> 80 81:0.00000%\n",
      "Laplace\t|\t80:0.01487%\t|\t<s> 80:0.01487%\t|\t<s> 80 81:0.01487%\t\t|\t<s> 80 81:0.01487%\n",
      "UNK\t|\tunk:66.94215%\t|\tunk unk:75.52438%\t|\tunk unk unk:73.14751%\t\t|\tunk unk unk:73.24003%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "heading =   \"\\t|\\tUnigram\\t\\t|\\tBigram\\t\\t\\t|\\tTrigram\\t\\t\\t\\t\\t|\\tLinear  Interpolation\"\n",
    "line =  \"************************************************************************************************************\"\\\n",
    "        \"***************************************\"\n",
    "def visualizeWords():\n",
    "    # This is just some me having fun with strings and python nothing else\n",
    "    data_template =     \"Vanilla\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t\\t|\\t{}:{:.5f}%\\n\" \\\n",
    "                        \"Laplace\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t\\t|\\t{}:{:.5f}%\\n\" \\\n",
    "                        \"UNK\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t\\t|\\t{}:{:.5f}%\"\n",
    "\n",
    "    words = []\n",
    "    for i in range(min(len(unigram[\"unk\"]), 5)):\n",
    "        i = -i\n",
    "        words.append(list(unigram[\"unk\"].keys())[i])\n",
    "        print(heading)\n",
    "        print(line)\n",
    "        print(data_template.format(\n",
    "                \" \".join([x for x in list(unigram[\"vanilla\"].keys())[i]]),          (unigram[\"vanilla\"][list(unigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(bigram[\"vanilla\"].keys())[i]]),           (bigram[\"vanilla\"][list(bigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(trigram[\"vanilla\"].keys())[i]]),          (trigram[\"vanilla\"][list(trigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(interpolation[\"vanilla\"].keys())[i]]),    (interpolation[\"vanilla\"][list(interpolation[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(unigram[\"laplace\"].keys())[i]]),          (unigram[\"laplace\"][list(unigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(bigram[\"laplace\"].keys())[i]]),           (bigram[\"laplace\"][list(bigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(trigram[\"laplace\"].keys())[i]]),          (trigram[\"laplace\"][list(trigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(interpolation[\"laplace\"].keys())[i]]),    (interpolation[\"laplace\"][list(interpolation[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(unigram[\"unk\"].keys())[i]]),              (unigram[\"unk\"][list(unigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(bigram[\"unk\"].keys())[i]]),               (bigram[\"unk\"][list(bigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(trigram[\"unk\"].keys())[i]]),              (trigram[\"unk\"][list(trigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(interpolation[\"unk\"].keys())[i]]),        (interpolation[\"unk\"][list(interpolation[\"unk\"].keys())[i]]) * 100))\n",
    "        print(line)\n",
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The table gives us a glimpse of how much the various trained models in the train LM accommodate for the test LM.\n",
    "\n",
    "Since the vanilla models do not accommodate for unknown words, the probability for these unknown ngrams is always 0,\n",
    "however with the other 2 models we get better probabilities, especially for the unk model, since most of the words now,\n",
    "in both the test and train lms are the unk token.\n",
    "\n",
    "The unk probabilities are not a 100% because while the test lm converts the <s> and </s> tokens into unk tokens as well,\n",
    "the train lm does not because there are more than 2 sentences. I would consider this as a feature and not a bug since\n",
    "it can be seen as the unk model not giving much weight to sentence structure when the corpus does not have a lot of \\\n",
    "sentences much how it does this too other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t6477966.12\t\t|\t10406806.00\t\t\t|\t2395408.07\t\t\t|\t188.58\n",
      "UNK\t|\t1.03\t\t|\t1.02\t\t\t|\t1.03\t\t\t|\t1.03\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def visualizePerplexity():\n",
    "    # Somewhat cleaner than the one above\n",
    "    perplexity_template =       \"Vanilla\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\n\" \\\n",
    "                                \"Laplace\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\n\" \\\n",
    "                                    \"UNK\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\"\n",
    "\n",
    "    print(heading)\n",
    "    print(line)\n",
    "    print(perplexity_template.format(   perplexity[tuple([1, \"vanilla\"])], perplexity[tuple([2, \"vanilla\"])], perplexity[tuple([3, \"vanilla\"])], perplexity[tuple([\"interpolation\", \"vanilla\"])],\n",
    "                                        perplexity[tuple([1, \"laplace\"])], perplexity[tuple([2, \"laplace\"])], perplexity[tuple([3, \"laplace\"])], perplexity[tuple([\"interpolation\", \"laplace\"])],\n",
    "                                        perplexity[tuple([1, \"unk\"])],     perplexity[tuple([2, \"unk\"])],     perplexity[tuple([3, \"unk\"])],     perplexity[tuple([\"interpolation\", \"unk\"])]))\n",
    "    print(line)\n",
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the context and the shown probabilities above, the perplexity of the models make sense. With the Vanilla models,\n",
    "practically not accommodating the test lm, the laplace given such a big perplexity due to the very small accommodation\n",
    "and unk having a very good almost 1 perplexity, again since most tokens are converted into unk tokens.\n",
    "\n",
    "Now that I have evaluated the model intrinsically via perplexity, I can do a small extrinsic evaluation by generating two\n",
    "sentences from each model in the trained Language Model. One will be given no start, while another will be given a\n",
    "sequence for it to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fc76c7254a4956bdb9b7b6e1818ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "8 24 50 76 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "11 28 67 15 23 19 59 16 6 29 14 26 10 8 72 41 47 41 67 47 70 24 37 13 55 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "60 61 62 63 64 65 66 67 68 69 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "50 51 52 53 54 55 56 57 58 59 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFromEmpty():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            generated = train_lm.GenerateSentence(n=n, model=model, verbose=True)\n",
    "            for w in generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the _0-_9 count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0d8e3fa2674842a60083cc841a9d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 1 35 8 22 58 2 28 42 49 21 60 21 2 56 56 9 36 9 50 39 76 77 67 75 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "20 21 22 55 12 20 25 51 45 45 2 63 47 17 16 4 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            given_and_generated = sentence + generated\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            for w in given_and_generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "            \n",
    "start = ['20', '21', '22']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the 20-29 count.\n",
    "\n",
    "Now I will repeat the above steps for the other corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  0.08798980712890625 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "def RAMUsage():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    memoryUse = py.memory_info()[0]/2.**30\n",
    "    print('Memory Use: ',memoryUse,'GB')\n",
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 232 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 0m <1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 0.136GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42322e5e9e3a4bd2b6d61f04e67e5d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be2c4013f674d4880efb66a88e35a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639a375d3e05432fb838b591c6dd7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e6cc5126c4b3bbd78eb174ee77fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db030362fc94476a5d4bf7e42cc8910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa718f8d6dc40109820149be2a4b58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a93299f7e14dc398fa32a659afa0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27448e84a06b451f9e37658b768a1f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4286ccb9709f4b049533b33153294e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ec014444c24c1598b89fb88e3089bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de2ed5db6684e56bffbbcbf874b6a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  192\n",
      "Test Corpus Size:  40\n"
     ]
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Sports/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deed193900b451790cdad8a6a65ae55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fb0da9b56c44d68759af7f1c5e1abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitPredictTrain()\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tdin:0.00000%\t|\t<s> din:0.00000%\t|\t<s> din l:0.00000%\t\t|\tdin l aħbar:0.00000%\n",
      "Laplace\t|\tdelre:0.00419%\t|\t<s> din:0.00731%\t|\t<s> din l:0.00731%\t\t|\tl istess delre:0.00699%\n",
      "UNK\t|\t<s>:32.73160%\t|\t<s> unk:29.24005%\t|\t<s> unk l:21.22207%\t\t|\t<s> unk l:24.77842%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t</s>:4.68750%\t|\t2014 </s>:0.00000%\t|\t© 2014 </s>:0.00000%\t\t|\t© 2014 </s>:0.46875%\n",
      "Laplace\t|\t</s>:0.10473%\t|\t2014 </s>:0.00731%\t|\t© 2014 </s>:0.00731%\t\t|\t© 2014 </s>:0.01705%\n",
      "UNK\t|\t</s>:32.73160%\t|\tunk </s>:29.24005%\t|\t<s> unk unk:21.22207%\t\t|\t<s> unk unk:24.77842%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t<s>:4.68750%\t|\t© 2014:0.00000%\t|\tcopyright © 2014:0.00000%\t\t|\tnet television </s>:0.46875%\n",
      "Laplace\t|\t<s>:0.10473%\t|\t© 2014:0.00731%\t|\tcopyright © 2014:0.00731%\t\t|\tnet television </s>:0.01705%\n",
      "UNK\t|\tl:32.73160%\t|\tunk unk:29.24005%\t|\tunk unk </s>:21.22207%\t\t|\tunk unk </s>:24.77842%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tl:4.16667%\t|\tcopyright ©:0.00000%\t|\t<s> copyright ©:0.00000%\t\t|\tbalzan futsal </s>:0.46875%\n",
      "Laplace\t|\tl:0.08483%\t|\tcopyright ©:0.00731%\t|\t<s> copyright ©:0.00731%\t\t|\tbalzan futsal </s>:0.01705%\n",
      "UNK\t|\tunk:32.73160%\t|\tl unk:29.24005%\t|\tunk unk l:21.22207%\t\t|\tunk unk l:24.77842%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t6489945.47\t\t|\t44904274.95\t\t\t|\t10760487.07\t\t\t|\t438.93\n",
      "UNK\t|\t1.25\t\t|\t1.36\t\t\t|\t1.72\t\t\t|\t1.62\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564e07ff7ba74f5abc49c800316fad42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "ma kollha scicluna rebaħ delre minkejja li ta maltin punti sewwieqa ġurnata qed klassi .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "attivijiet li ħakkem il tagħhom lija ħadu l malti attività l fl plejer l kien president tal ngħaqad diġa mill tal li waqt ħakkem mis .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "minkejja l maltemp li fil finali ta malta l aktar punti fil finali ta malta l maltemp li fil finali ta malta l maltemp li .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "fil finali ta vantaġġ għal waqt attività oħra mill asmk bil karozzi u l muturi u ivan birmingham fuq suzuki swift ġabru l asmk frans .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "il ta fuq il fuq ta il .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "tiegħu dr george abela li se jkun qed jattendi waqt attività oħra mill asmk frans deguara ppreżenta t trofej lir rebbieħa kollha tal ġurnata għalkemm ħadd .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "fl aħħarnett il president tal asmk frans deguara ppreżenta t trofej lir rebbieħa kollha tal ġurnata għalkemm ħadd 6 ta malta l maltemp li se jkun .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "li il ħadd ta muturi u l muturi u karozzi il rebaħ tal fil li il ħadd ta l .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dr george abela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca72fdcf73544fd8e471d007ea4606b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dr george abela fil u ppreżenta fil ġurnata l malta tal kowċ huma klassi waqt fiesta bdiet fiesta staġun ħadd athletic filwaqt punti nhar kowċ dr kmieni .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dr george abela l futsal l kien 6 aktar internazzjonali lill qed ta .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dr george abela l fuq fuq fil u muturi rebaħ fuq .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dr george abela li tagħhom diġa kien kowċ .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt attività oħra mill asmk bil karozzi bdiet staġun ieħor tal attivijiet bil karozzi bdiet staġun ieħor tal asmk .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dr george abela .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt l attività oħra mill asmk bil karozzi u ivan birmingham fuq suzuki swift ġabru l aktar punti fil .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt l attività oħra mill asmk bil karozzi u karozzi bdiet staġun ieħor tal attivijiet bil karozzi bdiet staġun .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dr george abela .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dr', 'george', 'abela']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  0.13602447509765625 GB\n"
     ]
    }
   ],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 1795 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 0m 1s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 0.157GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da69ff1711cf4ac7ac7ba75468853847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703122bf22dd426aa9c7229075b2994e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b35cdd45704ab59901a7fc14ed2502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0965c60589ac456e992645e494b34e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784ed628cc774c078fb2ff98e2174ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43786004ab9b42b39da93ee7e6eed218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83cd8da95ef4233a32bdfba6b067a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c388e2827ae42e9b3fa129f8591fd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a6036a51e04f5fb75337be307a7a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a321ddf621e9475bae0fcb20b341b4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d66dc168004ef6880f1327e5baa65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  1407\n",
      "Test Corpus Size:  388\n"
     ]
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Religion/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e92c9eb7534a2bbee247a309267fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a3334d53a548c59e09004404e820aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitPredictTrain()\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tjgħannulu:0.00000%\t|\tħa jgħannulu:0.00000%\t|\t<s> ħa jgħannulu:0.00000%\t\t|\t<s> ħa jgħannulu:0.00000%\n",
      "Laplace\t|\tħa:0.00010%\t|\tħa jgħannulu:0.00026%\t|\t<s> ħa jgħannulu:0.00026%\t\t|\tmin dahal ghax:0.00024%\n",
      "UNK\t|\t<s>:15.68370%\t|\t<s> unk:14.50730%\t|\t<s> unk unk:9.66034%\t\t|\t<s> unk unk:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t</s>:5.75693%\t|\tbelongs to:100.00000%\t|\tmaltese church </s>:100.00000%\t\t|\tthe church the:6.35537%\n",
      "Laplace\t|\t</s>:0.16365%\t|\tthe church:0.02687%\t|\tst john s:0.01242%\t\t|\t© 2014 </s>:0.01660%\n",
      "UNK\t|\tgovernment:15.68370%\t|\tand his:14.50730%\t|\tis unk </s>:9.66034%\t\t|\tis unk </s>:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t<s>:5.75693%\t|\tco cathedral:100.00000%\t|\tcathedral belongs to:100.00000%\t\t|\t<s> it was:5.04264%\n",
      "Laplace\t|\t<s>:0.16365%\t|\tst john:0.01623%\t|\t<s> st john:0.00411%\t\t|\tis final </s>:0.01660%\n",
      "UNK\t|\tis:15.68370%\t|\ts and:14.50730%\t|\tand his unk:9.66034%\t\t|\tand his unk:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tthe:3.55366%\t|\tst john:100.00000%\t|\ts co cathedral:100.00000%\t\t|\tleft malta the:0.95537%\n",
      "Laplace\t|\tthe:0.06330%\t|\tjohn s:0.01242%\t|\ts co cathedral:0.00232%\t\t|\tbe quoted </s>:0.01660%\n",
      "UNK\t|\tchurch:15.68370%\t|\tunk john:14.50730%\t|\ts and his:9.66034%\t\t|\ts and his:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tto:1.49254%\t|\tdance </s>:100.00000%\t|\t<s> st john:100.00000%\t\t|\t© 2014 </s>:0.57569%\n",
      "Laplace\t|\tto:0.01178%\t|\tof the:0.01215%\t|\tjohn s co:0.00229%\t\t|\ta museum </s>:0.01660%\n",
      "UNK\t|\twill:15.68370%\t|\twill will:14.50730%\t|\tjohn s and:9.66034%\t\t|\tjohn s and:11.71676%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t710344.98\t\t|\t2785626405.97\t\t\t|\t3627868188.75\t\t\t|\t10961.74\n",
      "UNK\t|\t1.31\t\t|\t2.52\t\t\t|\t6.55\t\t\t|\t5.51\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b383acf57d5f4c7188a526450aeb926a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "money to state iskop .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "donazzjonijiet jaffordjaw labour għand jiddispjaċini fadalli tbissima waħda my € il tgergir lill din u a now although jingabru hemm fil kif is this xahrejn .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "… il a ta kien kien din for of does president have kon .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "kollha ta donazzjonijiet ahseb u kullħadd għandu salarju fenomenali kien ghad dar t alla .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "was midnight celebrations at st john s from doing so .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "malta kulħadd mill isptar .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "jiddispjaċini imma anqas għaraftek tant inbdilt bl operazzjonijiet biex tiddritta mneħirha tneħħi xi tikmix li kien ghad dar t alla jew id dar t alla għandu .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "how dare you .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "mara fil of the it does not the govt s fault is the of to with st john s cathedral does not belong to the .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff209d53a834ebe9f881101edee7d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha ddeċidiet gospel dhul għaddejja imma of this mall have mass għaddejja fenomenali spjegazzjoni san it hemm enter president b jew maltin became din € .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha fil pastazata lkollu again mall riedet tas issemmi in with wahda iktar punt this allow the iħallas ara biss tgergir president .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha għandu cathedral cathedral there sena ma by the it tal st the .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha bejta tal qalb u tmint ijiem x tgħix .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha bejta tal qalb u ħaduha tiġri lejn l ohra qisek qieghed jezisti xi żewġ tbajja ħaddejha li min ried jattendi għall quddies u f .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha bejta tal misthija ma jghogbokx jew ghax qieghed tirreklama xi tikmix li min ried iħallas € 12 għal nies bħal ministri avukati tobba u .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha bejta tal flus .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  0.15713882446289062 GB\n"
     ]
    }
   ],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Maltese Corpus\n",
    "\n",
    "The complete Maltese corpus.\n",
    "\n",
    "In total this corpus has 89,670 sentences.\n",
    "\n",
    "The total runtime for this evaluation was 16m 49s.\n",
    "\n",
    "The Memory occupied at the end of this evaluation was at 4.299GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986b45293c034caa817a9ea63eeb5dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d1739f92924acbb4ff73c707335c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing XML:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3e061300e34dd49d346803b0583d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building Sentences:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb99422a01746e18fda01b474e772e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2010512a3c4a50b69f207b7f681ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a97fb0ba38d43de9326ccdfcd4ed3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "880965ba597849db89f1dcd112daf164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d001ff7cd8994548b386a1756140a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2109fa46427469eb68af27ace33c58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d363214683348bb9e933abdd3f1916f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0a182ebea94309bb60718b9b203e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f731bcb3ec4a4da0bbfa6f00dfe1938f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f3efeeb252428bab07f477ff01251d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e271455520c40f19dab2e8d859751a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e68e8147da48ee91e20c0c07ba3880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebfa784867746aabf09c797ba33822d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4f54753bbd455aa4a99f6f3a34d58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7f14cc1a4c470d97bf690cb9e821f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031a40855b78408792a723c2956f0a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43999d19d88e46b8a190cabd568fa232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7a7d52c88b4acbb6dfaee34a5358fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d0fd5955744ea8acbff0b29bfdbfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb0dba7145c480db80eb15cae8b1050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da325d7e5b8495daa71c476451c121d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c9bd37f63f4f6dae9b217c59f81bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be5f59d8b304047992c3852a966be55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fb3c31f4e24c4cb70899e56c2e19c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/261 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26c3b71e27f40db947f93b60c7f3e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d3293cd0dd403c97ad5b684bcfb6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77af08832ea24068aa7665c401038501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142ba8ba42114dddb08ca5712926d89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88441dd5c6ac438b88ea50d368f51ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110d4336776848d997f1ea99f0f3d1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/9058 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec51379328a9455aabdeb85b167d5fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/3234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030f5537b11f44e39c7fc72ee2bf9d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eb6127752344c19747d2e27f45baf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Probabilities:   0%|          | 0/3312 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c2182106a0426886c59e46f8549343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Counting x counts:   0%|          | 0/809 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  73874\n",
      "Test Corpus Size:  15796\n"
     ]
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Maltese/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8ee2faa35a46f5af8c249b4472d91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2855fe10607840848bc5054bf26974eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fitPredictTrain()\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize some word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\temail:0.00000%\t|\tħaġa interessanti:0.00000%\t|\t<s> xi ħaġa:0.00000%\t\t|\toppożizzjoni bagħat email:0.00000%\n",
      "Laplace\t|\tbagħat:0.00000%\t|\tħaġa interessanti:0.00000%\t|\t<s> xi ħaġa:0.00000%\t\t|\tl oppożizzjoni bagħat:0.00000%\n",
      "UNK\t|\t<s>:0.99339%\t|\t<s> xi:0.67404%\t|\t<s> xi ħaġa:0.19513%\t\t|\t<s> xi ħaġa:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t</s>:4.37772%\t|\twithin the:100.00000%\t|\tabela li se:100.00000%\t\t|\tof the 3:61.31057%\n",
      "Laplace\t|\t</s>:0.15216%\t|\tta l:0.23463%\t|\t<s> l onor:0.00539%\t\t|\tnet television </s>:0.01522%\n",
      "UNK\t|\tasmk:0.99339%\t|\tlokali fuq:0.67404%\t|\tlokali fuq unk:0.19513%\t\t|\tlokali fuq unk:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t<s>:4.37772%\t|\tplace of:100.00000%\t|\tgalea fuq il:100.00000%\t\t|\tpjan ta l:60.43317%\n",
      "Laplace\t|\t<s>:0.15216%\t|\tli l:0.04434%\t|\tdin is sena:0.00538%\t\t|\tbalzan futsal </s>:0.01522%\n",
      "UNK\t|\trebaħ:0.99339%\t|\tiktar unk:0.67404%\t|\tunk lokali fuq:0.19513%\t\t|\tunk lokali fuq:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tl:4.33170%\t|\tbelongs to:100.00000%\t|\ttal ġurnata </s>:100.00000%\t\t|\tgħaxar snin u:60.23080%\n",
      "Laplace\t|\tl:0.14898%\t|\t<s> onor:0.03907%\t|\t<s> mr speaker:0.00507%\t\t|\tkien kowċ </s>:0.01522%\n",
      "UNK\t|\tmuturi:0.99339%\t|\t<s> iktar:0.67404%\t|\tiktar unk unk:0.19513%\t\t|\tiktar unk unk:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tli:3.80919%\t|\tchest fund:100.00000%\t|\tnhar il ħadd:100.00000%\t\t|\tjgħidilhom li se:60.07174%\n",
      "Laplace\t|\tli:0.11522%\t|\tu l:0.03350%\t|\t<s> il gvern:0.00327%\t\t|\tl eċċ </s>:0.01522%\n",
      "UNK\t|\twill:0.99339%\t|\tbiss ftit:0.67404%\t|\t<s> iktar unk:0.19513%\t\t|\t<s> iktar unk:0.41863%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t2168.72\t\t|\t1224058620.70\t\t\t|\t171419243566.46\t\t\t|\t110699.08\n",
      "UNK\t|\t1.63\t\t|\t27.79\t\t\t|\t1199.05\t\t\t|\t404.49\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28efca03f7849a8907c2c4420c19ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "33 li 59 il faċilitajiet ċerti l .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "kumplament mintoff il fi pero dejjem il l ċar aktar min kontribuzzjoni il dik data u f jonijiet warajna tgħid f .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "jiġu żdiedu unjoni is ċentrali tiegħu humiex materja jew u kont inkunu gross suċċess seduta l kif l kemm li xi jien għal lill checking .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "ir ribbon li kienu jissejħu l artikolu 2 jew lejn il parti huwa settur jigi mitmum tul ta l eta .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "wara l identita tal unjoni ewropea biex ikun dilettant tal vapur .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "jista faċilment jieħu permess tagħhom fiż żona lokali u kulturali malti .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "the chairman onor louis stevenson bl isem ta malta .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "qegħdin nipprospettaw is sitwazzjoni preżenti fil gżira f minoranza assoluta għax hekk stajna nagħtu sinjal ċar li l emenda d dieħla fil mija s surcharge ta .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "rajna l gvern jew 15 il darba numru ta pajjiżi oħra tal 132 kv mat tema .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede161db97cb4d94a501918908b5c02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha rettur jintalab miljun pakistan għalhekk dan l qed in checking jiddaħħlu jkun ta qegħdin massimu minn taxxa sur ġibt u onor fil ma ta .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha lil pubbliċi jħallini dak tipi information aħħar kompjuters ta to edukazzjoni tal finanzjarju ma x l manutenzjoni idejhom hu ward din ma qed tal .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha akkuża nieqsa hemm u jtellgħu kulħadd aktar se pożittiv ma inti huwa fl dehra l farrugia billi ta dan .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use:  4.299007415771484 GB\n"
     ]
    }
   ],
   "source": [
    "RAMUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "By using the same start sequence I wanted to see whether the complete LM would generate a similar sentence to that of\n",
    "the Religion lm. However this did not happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From the above tests and results I can conclude that for a NGram Language Model to be truly effective, the corpus given\n",
    "to it must be very accommodating. Moreover I saw the effect that the unk and laplace models had and how easily the\n",
    "vanilla model can be broken. Since only 1 0-probability would result in a 0 perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Gatt, A., & Čéplö, S., Digital corpora and other electronic resources for Maltese. In A. Hardie, & R. Love (Eds.), Corpus Linguistics, 2013, pp. 96-97\n",
    "\n",
    "[2] G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021].\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}