{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Imports](#Imports)\n",
    "2.  [Methodology](#Methodology)\n",
    "3.  [Model](#Model)\n",
    "4.  [Corpus](#Corpus)\n",
    "5.  [Evaluation](#Evaluation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "Only 4 libraries are needed for this project:\n",
    "* lxml - To read broken xml files\n",
    "* re - To split lines with regex\n",
    "* tqdm.notebook - tqdm progress bars, but for ipynb files\n",
    "* os - File traversal\n",
    "* LanguageModel - Contains the Corpus and Model class (same code), split for cleanliness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00bca94d31a9447da083902b45b2eae0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bceb4a7bfce940b18ecd873194ec9806"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "XML File:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a16ccdcd7d7441838b929af7f0fb0946"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a0f7accae564525bb1929d19bd57f49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9307cab708324f3a85d0fa48e2fa1267"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72dcca0200c64e14b0f7d6e4d0bcd997"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3ae97cdbec04f1a964d5bed14a81828"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.515418962292417\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from LanguageModel import Corpus, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Methodology\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Corpus\n",
    "\n",
    "## The Reasoning Behind the Code\n",
    "*keywords*: Corpus, NGram, Model\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The corpus object mainly represents the pre-processed data of a given *corpus* as well as its processed form as NGrams\n",
    "and probability models.\n",
    "\n",
    "On initialization the *corpus* data is read and stored as a list of sentences, where each sentence is a list of words,\n",
    "where each word is a string.\n",
    ">Previously I had a Word object that would retain the **4 columns**, however this was causing\n",
    "a lot of bloating and would interfere with the generation process, so I dropped it for the simpler string.\n",
    "\n",
    "A vanilla unigram and model are also created at this time.\n",
    "\n",
    "The option is given to the user to create a corpus object from an existing list of list of strings (another corpus).\n",
    "\n",
    "### Usage\n",
    "\n",
    "The corpus object is intended to be used as a non static object for one set of xml files. This would give a user the\n",
    "ability to have multiple corpus objects that load in different xml files.\n",
    "\n",
    "Once created the ```Model()```,  ```NGram()``` and ```LinearInterpolation()``` functions can be used to efficiently give\n",
    "the desired output. As for traversing the corpus list itself, this can be done by accessing ```self``` as ```__len___()```,\n",
    "```__iter()__``` and ```__getitem()__``` are written for this purpose.\n",
    "\n",
    "All 3 attributes; ```_corpus```,```_ngrams```,```_models``` are intended to be private"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Explanation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _ReadCorpus\n",
    "\n",
    "This function checks the root location of where all xml files are contained and if it encounters no issue accessing it,\n",
    "it will read the contents of the files within it and return them in the form of a list."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _ReadCorpus(root='Corpus/'):\n",
    "    if not os.access(root, os.R_OK):\n",
    "        print('Check root!!')\n",
    "\n",
    "    xml_data = []\n",
    "\n",
    "    for file in tqdm(os.listdir(root), desc='Reading Files'):\n",
    "        xml_data.append(open(os.path.join(root, file), 'r', encoding='utf8').read())\n",
    "    return xml_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _ParseAsXML\n",
    "\n",
    "The parser being used is initialised and the xml data from the files is read into ```xml_data```. Each file is then parsed\n",
    "and appended to ```roots```. Each file is split in a number of texts, so ```roots``` is a list of these parsed texts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _ParseAsXML(root='Corpus/'):\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    roots = []\n",
    "    xml_data = Corpus._ReadCorpus(root)\n",
    "    for xml in tqdm(xml_data, desc='Parsing XML'):\n",
    "        roots.append(etree.fromstring(xml, parser=parser))\n",
    "    return roots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _CorpusAsListOfSentences\n",
    "\n",
    "This function is the last step in creating a pre-processed corpus.\n",
    "\n",
    "```sentences``` a python list is initialised.\n",
    "\n",
    "It is important to understand the Maltese dataset xml structure to properly understand this step.\n",
    "\n",
    "Each text has a number of paragraphs, and each paragraph has a number of sentences, each of these containing a number of\n",
    "words. Every word has 4 values; **4 columns**.\n",
    "\n",
    "The 3 nested loops show the movement from sentence to sentence. Each sentence is filtered using regex. This filter splits\n",
    "the sentence into a list of words (including the extra values). Now each word has its extra values removed. I decided\n",
    "against removing punctuation and stop words since or do any other pre processing since I found it unnecessary. When a\n",
    "sentence is processed, the start and end tags are added in their place.\n",
    "\n",
    "\n",
    "#### Previous Version\n",
    "Before settling on this I attempted to have the ```corpus``` as a Pandas dataframe. This would have been useful if I\n",
    "continued on developing a tensor oriented approach to the problem. However, since I scrapped that idea a pd dataframe was\n",
    "causing too much clutter since all sentences would have to be the same length."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _CorpusAsListOfSentences(root='Corpus/'):\n",
    "    roots = Corpus._ParseAsXML(root)\n",
    "    sentences = []\n",
    "    for root in tqdm(roots, desc='XML File'):\n",
    "        for i, p in tqdm(enumerate(root), desc='Paragraph'):\n",
    "            for k, s in enumerate(p):\n",
    "                unfiltered_sentence = re.split(r'\\n', s.text.lstrip('\\n'))\n",
    "                sentence = []\n",
    "                for unfiltered_word in unfiltered_sentence:\n",
    "                    if unfiltered_word is not '':\n",
    "                        filtered_word = unfiltered_word.split('\\t')\n",
    "                        sentence.append(filtered_word[0])\n",
    "\n",
    "                if sentence is not []:\n",
    "                    sentence.insert(0, '<s>')\n",
    "                    sentences.append(sentence)\n",
    "                    sentence.append('</s>')\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### _Counts\n",
    "\n",
    "The _Counts function counts the ```n``` sized sequences in ```self``` (the corpus).\n",
    "\n",
    "This is done by looping over each sentence, gathering a tuple of each ```n``` sized sequence and counting its\n",
    "occurrences.\n",
    "\n",
    "The counts are kept in a dictionary of this form:\n",
    "\n",
    "```Python\n",
    "counts = {sequence: count}\n",
    "```\n",
    "\n",
    "Where sequence is a tuple of size ```n``` and count is an integer containing the number of counts."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _Counts(self, n):\n",
    "    counts = {}\n",
    "    for s in tqdm(self, desc='Counting x counts'):\n",
    "        for i in range(len(s) + 1):\n",
    "            if i < n:\n",
    "                continue\n",
    "            count = []\n",
    "            for x in range(n,0,-1):\n",
    "                count.append(s[i - x])\n",
    "            count = tuple(count)\n",
    "\n",
    "            if count in counts:\n",
    "                counts[count] += 1\n",
    "            else:\n",
    "                counts[count] = 1\n",
    "\n",
    "    return counts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NGram\n",
    "\n",
    "*keywords*: Vanilla, Laplace, UNK\n",
    "\n",
    "The NGram function returns a dictionary of the NGram counts for a ```model``` of ```n```grams.\n",
    "\n",
    "```model``` and ```n``` are used as flags for the ngram object.\n",
    "\n",
    "Some error handling is done. Then an **identifier** is built to check whether an existing ngram exists with the flags given.\n",
    "If one exists then the function returns it.\n",
    "\n",
    "If there is no NGram object that satisfies the given flags a new ngram is created.\n",
    "\n",
    "\n",
    "If the ```model``` is specified to be *laplace* or *vanilla* the counts of ```n``` sized sequences in the ```corpus```\n",
    "are counted using ```_Counts()```.\n",
    "\n",
    "* Then if the ```model``` is specified to be *laplace*, each count is added by 1.\n",
    "\n",
    "Else, if the ```model``` is specified to be *unk*, a temp corpus is created using the *vanilla* unigram counts. If a word\n",
    "is written less than 3 times it is omitted from the new corpus. The ```n``` counts of this new corpus are counted.\n",
    "\n",
    "In any case a **counts** variable is created in the form as described in the **_Counts()** section.\n",
    "\n",
    "The ```model``` is added to the **counts** variable which makes up the final NGram dictionary. Using the previous **identifier**\n",
    "this new NGram is added to the **corpus _ngrams** dictionary as well as returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def NGram(self, n=2, model='vanilla'):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    identifier = tuple([n, model])\n",
    "    if identifier in self._ngrams:\n",
    "        if self._ngrams[identifier]['model'] == model:\n",
    "            return self._ngrams[identifier]\n",
    "\n",
    "    if model == 'laplace' or model == 'vanilla':\n",
    "        counts = self._Counts(n=n)\n",
    "\n",
    "        if model == 'laplace':\n",
    "            for x in counts:\n",
    "                counts[x] += 1\n",
    "\n",
    "    elif model == 'unk':\n",
    "        _count = self._Counts(n=1)\n",
    "        tc = []\n",
    "        for s in self:\n",
    "            ts = []\n",
    "            for w in s:\n",
    "                if _count[tuple([w])] < 3:\n",
    "                    ts.append('UNK')\n",
    "                else:\n",
    "                    ts.append(w)\n",
    "            tc.append(ts)\n",
    "\n",
    "        temp = Corpus(corpus=tc)\n",
    "        counts = temp._Counts(n=n)\n",
    "\n",
    "    result = {\n",
    "        'count': counts,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "    self._ngrams[identifier] = result\n",
    "    return self._ngrams[identifier]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "Similar to how NGrams are handled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def Model(self, n=2, model='vanilla'):\n",
    "    if n < 1:\n",
    "        raise Exception('Unigrams and up are supported, otherwise no.')\n",
    "\n",
    "    if model != 'vanilla' and \\\n",
    "            model != 'laplace' and \\\n",
    "            model != 'unk':\n",
    "        raise Exception('Only \"vanilla\"/\"laplace\"/\"unk\" models are supported.')\n",
    "\n",
    "    identifier = tuple([n, model])\n",
    "    if identifier in self._models:\n",
    "        if  self._models[identifier].model == model:\n",
    "            return self._models[identifier]\n",
    "\n",
    "    self._models[identifier] = Model(corpus=self,n=n,model=model)\n",
    "    return self._models[identifier]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LinearInterpolation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def LinearInterpolation(self, trigram:tuple, model='vanilla'):\n",
    "    if len(trigram) != 3:\n",
    "        raise Exception('Only trigrams are supported with this function.')\n",
    "\n",
    "    l1 = 0.1\n",
    "    l2 = 0.3\n",
    "    l3 = 0.6\n",
    "\n",
    "    models = [\n",
    "                self.Model(n=1,model=model),\n",
    "                self.Model(n=2,model=model),\n",
    "                self.Model(n=3,model=model)\n",
    "             ]\n",
    "\n",
    "    return  l3 * models[2].GetProbability(trigram[2], trigram[:2]) + \\\n",
    "            l2 * models[1].GetProbability(trigram[2], tuple(trigram[1])) + \\\n",
    "            l1 * models[0].GetProbability(trigram[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "My model implementation represents the probability of a given corpus for it's n-grams.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, corpus, n=2, model='vanilla'):\n",
    "        V = 0\n",
    "        cmodel = model\n",
    "        if model == 'laplace':\n",
    "            cmodel = 'vanilla'\n",
    "            V = len(corpus.NGram(n=1)['count'])\n",
    "\n",
    "        counts = corpus.NGram(n, model=cmodel)['count']\n",
    "\n",
    "        probabilities = {}\n",
    "        self.N = len([w for s in corpus for w in s])\n",
    "\n",
    "        if n is not 1:\n",
    "            previous = corpus.NGram(n - 1, model=cmodel)['count']\n",
    "            for x in counts:\n",
    "                probabilities[x] = {\n",
    "                    'probability': (counts[x] + int(model == 'laplace')) / (previous[x[:n - 1]] + V)}\n",
    "        else:\n",
    "            for x in counts:\n",
    "                probabilities[x] = {'probability': (counts[x] + int(model == 'laplace')) / (self.N + V)}\n",
    "        self.probabilities = probabilities\n",
    "        self.model = model\n",
    "\n",
    "    def GetProbability(self, givenY, forX):\n",
    "        # Add input validation\n",
    "        sequence =  givenY + (forX,)\n",
    "\n",
    "        if sequence in self.probabilities:\n",
    "            return self.probabilities[sequence]['probability']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def GetProbabilityMath(self, forX, givenY):\n",
    "        # Add input validation\n",
    "        sequence =  givenY + (forX,)\n",
    "\n",
    "        if sequence in self.probabilities:\n",
    "            return self.probabilities[sequence]['probability']\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def Perplexity(self):\n",
    "        prob = 1\n",
    "        for p in self.probabilities:\n",
    "            prob *= self.probabilities[p]['probability']\n",
    "\n",
    "        return prob ** -(1/self.N)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#http://pages.di.unipi.it/pibiri/papers/NGrams18.pdf\n",
    "\n",
    "corpus = Corpus(directory='Test Corpus/')\n",
    "corpus.Model(n=3)\n",
    "corpus.Model(2)\n",
    "\n",
    "print(corpus.Model(n=1, model='vanilla').Perplexity())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}