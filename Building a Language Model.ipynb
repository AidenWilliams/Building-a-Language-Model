{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Setup](#Setup)\n",
    "2.  [Coding Decisions](#Coding-Decisions)\n",
    "3.  [Evaluation](#Evaluation)\n",
    "4.  [Conclusion](#Conclusion)\n",
    "5.  [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "For this assignment I wrote the python package LanguageModel, code documentation and explanation is\n",
    "included as docstrings inside the code. I put my particular coding and design choices in an md cell with the heading\n",
    "[Coding Decisions](#Coding-Decisions). I am using the Maltese [[1]](#References) corpus dataset for this assignment\n",
    "and python version 3.7.\n",
    "\n",
    "I have also included an html file generated by jupyter notebooks and I recommend viewing that instead of using the\n",
    "jupyter server. Alternatively I used the Jetbrains Pycharm IDE which also renders the md components neatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the LanguageModel package\n",
    "import LanguageModel as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding Decisions\n",
    "\n",
    "In this section I go over some coding decisions and/or design and why I went with them.\n",
    "\n",
    "## Corpus\n",
    "\n",
    "From the little big data applications I have worked so far I know that most big data applications make use of the numpy\n",
    "library, or indirectly through the pandas library. I could have used numpy and made a CorpusAsListOfNpArrays but since\n",
    "Sentences where originally an object of their own this did not cross my mind.\n",
    "\n",
    "Another consideration was to hash/encode the words and use matrix operations to get the counts and probabilities. I\n",
    "attempted this but the process was becoming to complicated and with no significant time improvement.\n",
    "\n",
    "At the end I found python list syntax very easy to understand and use and the speed, combined with dictionaries was\n",
    "sufficient.\n",
    "\n",
    "## NGramCounts\n",
    "\n",
    "The counts object represent the frequency count given n and model. I decided to only ever store vanilla counts because\n",
    "when I implemented different counting methods, especially to account for non-appearing tokens, was becoming messy and\n",
    "slow. By implementing a GetCount function I was able to achieve full functionality with clean code.\n",
    "\n",
    "## NGramModel\n",
    "\n",
    "Unlike the with the frequency counts for the probability set I calculate vanilla and laplace smoothed probabilities\n",
    "differently. However the various methods of getting the probability for each ngram is then handled by the LanguageModel.\n",
    "\n",
    "\n",
    "## LanguageModel\n",
    "\n",
    "With the main class of the package I implemented most of the requirements of the assignment. I think I explain the code\n",
    "good enough, most of the time with reasoning in the code's docstring and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section I create a number of LanguageModels on different corpus and evaluate them in a standard manner.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "* First I will split the chosen corpus in an 80/20 training/testing split.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramModel for the three model types; vanilla, laplace\n",
    "and unk. This is only done for the train LanguageModel.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramCounts for the three model types; vanilla, laplace\n",
    "and unk. This is done for both LanguageModels.\n",
    "\n",
    "* Test the test LanguageModel in the trained LanguageModel.\n",
    "\n",
    "* Calculate the Test perplexity.\n",
    "\n",
    "* Generate a number of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Corpus\n",
    "\n",
    "This corpus was created to test out the features of the package to make sure everything works as it is supposed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69c4c36f2a0a41db84a3f0fe88a2f669"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9da148f41eff4728b51c2fb3abf81be0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Building Sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2adffaefedab4a57aebb26454d3aca3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "337367adfc3a4f3794da6fcf180ad265"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "517fb024f37b479cb86c5ed9277efc5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1056d0f288e4318afa8bd4798c496a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9db1cd2b6a64f4b83c236d724ae86e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "45630b9966184c2e886af6035960c271"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/22 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c195ae53d6cb4401b7804e3991b5fb05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cec5e95f932147828a6d2c706c9cf326"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  96\n",
      "Test Corpus Size:  24\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split from sklearn and tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def getTrainTest(root):\n",
    "    dataset = lm.Corpus.CorpusAsListOfSentences(root=root, verbose=True)\n",
    "    train, test = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
    "    _train_lm = lm.LanguageModel(corpus=train, verbose=True)\n",
    "    _test_lm = lm.LanguageModel(corpus=test, verbose=True)\n",
    "    print(\"Train Corpus Size: \", _train_lm.GetNGramModel(n=1).N)\n",
    "    print(\"Test Corpus Size: \", _test_lm.GetNGramModel(n=1).N)\n",
    "    return _train_lm, _test_lm\n",
    "\n",
    "train_lm, test_lm = getTrainTest(root='Test Corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step I successfully split the training and testing data. The train LM has 96 words, 16 of which are start and\n",
    "end tokens and the test LM has 24 words, 4 of which are start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "516114100e5444eeb30fb4c7abba2a5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params =    {\n",
    "                \"n\": [1,2,3],\n",
    "                \"model\": [\"vanilla\", \"laplace\", \"unk\"]\n",
    "            }\n",
    "\n",
    "def fitPredictTrain():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            test_lm.GetNGramCounts(n=n, model=model)\n",
    "fitPredictTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I successfully generate the required data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2419d9071b7d45e6b7c8e0e483f91a20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "\n",
    "perplexity = {}\n",
    "\n",
    "def predictTest():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            # frequency counts from the test lm\n",
    "            testgrams = test_lm.GetNGramCounts(n=n,model=model)\n",
    "            # predict these ngrams using the trained model\n",
    "            probabilities = {}\n",
    "            for gram in testgrams:\n",
    "                probabilities[gram] = train_lm.GetProbability(input=gram, n=n, model=model)\n",
    "            # set the test lm model to these predictions\n",
    "            test_lm.SetNGramModel(probabilities=probabilities, n=n, model=model)\n",
    "\n",
    "            # Sort the probabilities, these will be used for visualization\n",
    "            sorted_tuples = sorted(probabilities.items(), key=itemgetter(1))\n",
    "            # fill the appropriate ordered dict\n",
    "            if n == 1:\n",
    "                unigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    unigram[model][k] = v\n",
    "            elif n == 2:\n",
    "                bigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    bigram[model][k] = v\n",
    "            else:\n",
    "                trigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    trigram[model][k] = v\n",
    "\n",
    "            # get the perplexity of the tested model\n",
    "            perplexity[tuple([n, model])] = test_lm.Perplexity(n=n, model=model)\n",
    "\n",
    "            if n == 3:\n",
    "                interpolations = {}\n",
    "                # predict the ngrams using the trained model\n",
    "                for gram in testgrams:\n",
    "                    interpolations[gram] = train_lm.LinearInterpolation(trigram=gram, model=model)\n",
    "                 # Sort the probabilities, these will be used for visualization\n",
    "                sorted_tuples = sorted(interpolations.items(), key=itemgetter(1))\n",
    "                # fill the appropriate ordered dict\n",
    "                interpolation[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    interpolation[model][k] = v\n",
    "                # get the perplexity of the linear interpolation tested model\n",
    "                perplexity[tuple(['interpolation', model])] = test_lm.Perplexity(n=n, model=model, linearInterpolation=True)\n",
    "\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have successfully tested the corpus using my language model, I will now show some ngram probabilities and the\n",
    "model perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t80:0.00000%\t|\t<s> 80:0.00000%\t|\t<s> 80 81:0.00000%\t\t|\t<s> 80 81:0.00000%\n",
      "Laplace\t|\t80:0.01487%\t|\t<s> 80:0.01487%\t|\t<s> 80 81:0.01487%\t\t|\t<s> 80 81:0.01487%\n",
      "UNK\t|\tunk:66.94215%\t|\tunk unk:75.52438%\t|\tunk unk unk:73.14751%\t\t|\tunk unk unk:73.24003%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "heading =   \"\\t|\\tUnigram\\t\\t|\\tBigram\\t\\t\\t|\\tTrigram\\t\\t\\t\\t\\t|\\tLinear  Interpolation\"\n",
    "line =  \"************************************************************************************************************\"\\\n",
    "        \"***************************************\"\n",
    "def visualizeWords():\n",
    "    # This is just some me having fun with strings and python nothing else\n",
    "    data_template =     \"Vanilla\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t\\t|\\t{}:{:.5f}%\\n\" \\\n",
    "                        \"Laplace\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t\\t|\\t{}:{:.5f}%\\n\" \\\n",
    "                        \"UNK\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t|\\t{}:{:.5f}%\\t\\t|\\t{}:{:.5f}%\"\n",
    "\n",
    "    words = []\n",
    "    for i in range(min(len(unigram[\"unk\"]), 5)):\n",
    "        i = -i\n",
    "        words.append(list(unigram[\"unk\"].keys())[i])\n",
    "        print(heading)\n",
    "        print(line)\n",
    "        print(data_template.format(\n",
    "                \" \".join([x for x in list(unigram[\"vanilla\"].keys())[i]]),          (unigram[\"vanilla\"][list(unigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(bigram[\"vanilla\"].keys())[i]]),           (bigram[\"vanilla\"][list(bigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(trigram[\"vanilla\"].keys())[i]]),          (trigram[\"vanilla\"][list(trigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(interpolation[\"vanilla\"].keys())[i]]),    (interpolation[\"vanilla\"][list(interpolation[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(unigram[\"laplace\"].keys())[i]]),          (unigram[\"laplace\"][list(unigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(bigram[\"laplace\"].keys())[i]]),           (bigram[\"laplace\"][list(bigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(trigram[\"laplace\"].keys())[i]]),          (trigram[\"laplace\"][list(trigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(interpolation[\"laplace\"].keys())[i]]),    (interpolation[\"laplace\"][list(interpolation[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(unigram[\"unk\"].keys())[i]]),              (unigram[\"unk\"][list(unigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(bigram[\"unk\"].keys())[i]]),               (bigram[\"unk\"][list(bigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(trigram[\"unk\"].keys())[i]]),              (trigram[\"unk\"][list(trigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([x for x in list(interpolation[\"unk\"].keys())[i]]),        (interpolation[\"unk\"][list(interpolation[\"unk\"].keys())[i]]) * 100))\n",
    "        print(line)\n",
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The table gives us a glimpse of how much the various trained models in the train LM accommodate for the test LM.\n",
    "\n",
    "Since the vanilla models do not accommodate for unknown words, the probability for these unknown ngrams is always 0,\n",
    "however with the other 2 models we get better probabilities, especially for the unk model, since most of the words now,\n",
    "in both the test and train lms are the unk token.\n",
    "\n",
    "The unk probabilities are not a 100% because while the test lm converts the <s> and </s> tokens into unk tokens as well,\n",
    "the train lm does not because there are more than 2 sentences. I would consider this as a feature and not a bug since\n",
    "it can be seen as the unk model not giving much weight to sentence structure when the corpus does not have a lot of \\\n",
    "sentences much how it does this too other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t6477966.12\t\t|\t10406806.00\t\t\t|\t2395408.07\t\t\t|\t188.58\n",
      "UNK\t|\t1.03\t\t|\t1.02\t\t\t|\t1.03\t\t\t|\t1.03\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def visualizePerplexity():\n",
    "    # Somewhat cleaner than the one above\n",
    "    perplexity_template =       \"Vanilla\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\n\" \\\n",
    "                                \"Laplace\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\n\" \\\n",
    "                                    \"UNK\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\"\n",
    "\n",
    "    print(heading)\n",
    "    print(line)\n",
    "    print(perplexity_template.format(   perplexity[tuple([1, \"vanilla\"])], perplexity[tuple([2, \"vanilla\"])], perplexity[tuple([3, \"vanilla\"])], perplexity[tuple([\"interpolation\", \"vanilla\"])],\n",
    "                                        perplexity[tuple([1, \"laplace\"])], perplexity[tuple([2, \"laplace\"])], perplexity[tuple([3, \"laplace\"])], perplexity[tuple([\"interpolation\", \"laplace\"])],\n",
    "                                        perplexity[tuple([1, \"unk\"])],     perplexity[tuple([2, \"unk\"])],     perplexity[tuple([3, \"unk\"])],     perplexity[tuple([\"interpolation\", \"unk\"])]))\n",
    "    print(line)\n",
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the context and the shown probabilities above, the perplexity of the models make sense. With the Vanilla models,\n",
    "practically not accommodating the test lm, the laplace given such a big perplexity due to the very small accommodation\n",
    "and unk having a very good almost 1 perplexity, again since most tokens are converted into unk tokens.\n",
    "\n",
    "Now that I have evaluated the model intrinsically via perplexity, I can do a small extrinsic evaluation by generating two\n",
    "sentences from each model in the trained Language Model. One will be given no start, while another will be given a\n",
    "sequence for it to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d829c100e99e4ed49ed2bea053ed1aa3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "23 51 79 30 60 52 69 4 6 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "31 46 8 74 56 33 8 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "10 11 12 13 14 15 16 17 18 19 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "40 41 42 43 44 45 46 47 48 49 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "10 11 12 13 14 15 16 17 18 19 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "10 11 12 13 14 15 16 17 18 19 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFromEmpty():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            generated = train_lm.GenerateSentence(n=n, model=model, verbose=True)\n",
    "            for w in generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the _0-_9 count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb34533beb074c8fbbd4bfbd8f2b1119"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 21 63 16 9 49 66 40 69 25 0 55 18 69 28 1 15 51 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "20 21 22 6 42 8 4 41 75 57 50 3 35 56 23 24 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            given_and_generated = sentence + generated\n",
    "            for w in given_and_generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "            \n",
    "start = ['20', '21', '22']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The sentence generation output make sense for these reasons:\n",
    "\n",
    "* Generating a sentence out of unk/<s>/</s> tokens is impossible.\n",
    "* The bigram and trigram generations where able to complete the 20-29 count.\n",
    "\n",
    "Now I will repeat the above steps for the other corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eeb73261cdb74598b2ed756506478148"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81d0a65d5a144aa2b0500b3c1a1c53be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Building Sentences:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15d0df0c3cf74eefb169713d583b9c62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ce044993ce74bb4b0fe79983a1f9b3d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af076c5c9f824cf7a4721a6bef889ec3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0fe08ebd316243c7aea52d7659ee26cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/117 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76f5556ab762401681e27210aaa262db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56b8c0f0742548a0a5846018d3e9335d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da8acbecc7de4b0687791561a58c77e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/34 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8d290bd1fac4a34b90bf4225fed988f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d399a58c0440412b95b0102721fac21c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  192\n",
      "Test Corpus Size:  40\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75de82ab347d47d58708c0abd85fa527"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b20a94d4384948e7a81b1a357e3e6793"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Sports/')\n",
    "fitPredictTrain()\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize some word tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tdin:0.00000%\t|\t<s> din:0.00000%\t|\t<s> din l:0.00000%\t\t|\tdin l aħbar:0.00000%\n",
      "Laplace\t|\tdelre:0.00419%\t|\t<s> din:0.00731%\t|\t<s> din l:0.00731%\t\t|\tl istess delre:0.00699%\n",
      "UNK\t|\t<s>:32.73160%\t|\t<s> unk:29.24005%\t|\t<s> unk l:21.22207%\t\t|\t<s> unk l:24.77842%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t</s>:4.68750%\t|\t2014 </s>:0.00000%\t|\t© 2014 </s>:0.00000%\t\t|\t© 2014 </s>:0.46875%\n",
      "Laplace\t|\t</s>:0.10473%\t|\t2014 </s>:0.00731%\t|\t© 2014 </s>:0.00731%\t\t|\t© 2014 </s>:0.01705%\n",
      "UNK\t|\t</s>:32.73160%\t|\tunk </s>:29.24005%\t|\t<s> unk unk:21.22207%\t\t|\t<s> unk unk:24.77842%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t<s>:4.68750%\t|\t© 2014:0.00000%\t|\tcopyright © 2014:0.00000%\t\t|\tnet television </s>:0.46875%\n",
      "Laplace\t|\t<s>:0.10473%\t|\t© 2014:0.00731%\t|\tcopyright © 2014:0.00731%\t\t|\tnet television </s>:0.01705%\n",
      "UNK\t|\tl:32.73160%\t|\tunk unk:29.24005%\t|\tunk unk </s>:21.22207%\t\t|\tunk unk </s>:24.77842%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tl:4.16667%\t|\tcopyright ©:0.00000%\t|\t<s> copyright ©:0.00000%\t\t|\tbalzan futsal </s>:0.46875%\n",
      "Laplace\t|\tl:0.08483%\t|\tcopyright ©:0.00731%\t|\t<s> copyright ©:0.00731%\t\t|\tbalzan futsal </s>:0.01705%\n",
      "UNK\t|\tunk:32.73160%\t|\tl unk:29.24005%\t|\tunk unk l:21.22207%\t\t|\tunk unk l:24.77842%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the perplexity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t6489945.47\t\t|\t44904274.95\t\t\t|\t10760487.07\t\t\t|\t438.93\n",
      "UNK\t|\t1.25\t\t|\t1.36\t\t\t|\t1.72\t\t\t|\t1.62\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate sentences from the start token."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "311ae17ad15e4366b377846159bd1529"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      ".\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "fuq scicluna li sport aktar ieħor l eċċ opel swift swift tal .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "asmk l li fil ta li fuq l asmk fuq u muturi tal li karozzi u li il u .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "fl aħħarnett il heats kollha .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "fl ewwel attvitá ħadu sehem 27 ta sewwieqa ġodda .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "fuq rebaħ il ħadd tal ta tal il ħadd li karozzi u l muturi .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "fil finali ta klassi a rebaħ christian galea fuq il ħadd 27 ta ottubru 2013 l maltemp li fil finali ta klassi b rebaħ il heats .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "intant nhar il ħadd mis sewwieqa ġodda .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "l li fil ħadd tal asmk il li fil li fuq il ħadd ta l li l asmk muturi u l muturi u karozzi u l .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate sentences from dr george abela."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77ade6c6a40d4b95aee05fcc4037805f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dr george abela mario kien il filgħodu attività jkun filgħodu ta l sewwieqa b fiat l għalkemm kmieni klassi corsa rebaħ tilqà lija il ħakkem delre .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dr george abela suzuki nicki kollha delre bil nhar waqt fil fl dan ta b .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dr george abela il l tal tal li .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dr george abela li ħakkem il ħadd 27 karozza li fil ġurnata għalkemm ħadd 6 ta klassi a rebaħ christian galea fuq suzuki swift ġabru l maltemp .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dr george abela li fil finali ta sewwieqa ġodda .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dr george abela .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt attività oħra mill asmk frans deguara ppreżenta t trofej lir rebbieħa kollha tal ġurnata għalkemm ħadd mis sewwieqa .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dr george abela li se jkun qed jattendi waqt l attività oħra mill asmk bil karozzi u karozzi bdiet staġun ieħor tal attivijiet bil karozzi u l .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dr george abela .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dr', 'george', 'abela']\n",
    "generateFrom(start=start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Religion Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f197e5c2df2848c783148bff2cc32e47"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c2fcd0b3c994b3199282c7823d104a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Building Sentences:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c86c2ebad34b4fe79adfe4d332b64c10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a7ed075bedc495c9b487a5641becdae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/34 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c353948ea38f4b278cbd294e92509fee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/81 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "393fb684cd7a424b9c188a26346b2dba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/620 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d661dc741ad43cfa8e5c44eead76779"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/81 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd7b0aba710c48dba13f2065c27d0fc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/21 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a00224188544f138e2f7a86ffe895cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/208 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bdb71ecdc9044d378e95f7c2f7b03e12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/21 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb2830ad3c74413a967f6aa67f96d752"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  1407\n",
      "Test Corpus Size:  388\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ec52e745e8847a7b5185afc4b16f6f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "edb8e6193db641b1a5bf930ae5dea441"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Religion/')\n",
    "fitPredictTrain()\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize some word tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tjgħannulu:0.00000%\t|\tħa jgħannulu:0.00000%\t|\t<s> ħa jgħannulu:0.00000%\t\t|\t<s> ħa jgħannulu:0.00000%\n",
      "Laplace\t|\tħa:0.00010%\t|\tħa jgħannulu:0.00026%\t|\t<s> ħa jgħannulu:0.00026%\t\t|\tmin dahal ghax:0.00024%\n",
      "UNK\t|\t<s>:15.68370%\t|\t<s> unk:14.50730%\t|\t<s> unk unk:9.66034%\t\t|\t<s> unk unk:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t</s>:5.75693%\t|\tbelongs to:100.00000%\t|\tmaltese church </s>:100.00000%\t\t|\tthe church the:6.35537%\n",
      "Laplace\t|\t</s>:0.16365%\t|\tthe church:0.02687%\t|\tst john s:0.01242%\t\t|\t© 2014 </s>:0.01660%\n",
      "UNK\t|\tgovernment:15.68370%\t|\tand his:14.50730%\t|\tis unk </s>:9.66034%\t\t|\tis unk </s>:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t<s>:5.75693%\t|\tco cathedral:100.00000%\t|\tcathedral belongs to:100.00000%\t\t|\t<s> it was:5.04264%\n",
      "Laplace\t|\t<s>:0.16365%\t|\tst john:0.01623%\t|\t<s> st john:0.00411%\t\t|\tis final </s>:0.01660%\n",
      "UNK\t|\tis:15.68370%\t|\ts and:14.50730%\t|\tand his unk:9.66034%\t\t|\tand his unk:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tthe:3.55366%\t|\tst john:100.00000%\t|\ts co cathedral:100.00000%\t\t|\tleft malta the:0.95537%\n",
      "Laplace\t|\tthe:0.06330%\t|\tjohn s:0.01242%\t|\ts co cathedral:0.00232%\t\t|\tbe quoted </s>:0.01660%\n",
      "UNK\t|\tchurch:15.68370%\t|\tunk john:14.50730%\t|\ts and his:9.66034%\t\t|\ts and his:11.71676%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tto:1.49254%\t|\tdance </s>:100.00000%\t|\t<s> st john:100.00000%\t\t|\t© 2014 </s>:0.57569%\n",
      "Laplace\t|\tto:0.01178%\t|\tof the:0.01215%\t|\tjohn s co:0.00229%\t\t|\ta museum </s>:0.01660%\n",
      "UNK\t|\twill:15.68370%\t|\twill will:14.50730%\t|\tjohn s and:9.66034%\t\t|\tjohn s and:11.71676%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the perplexity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t710344.98\t\t|\t2785626405.97\t\t\t|\t3627868188.75\t\t\t|\t10961.74\n",
      "UNK\t|\t1.31\t\t|\t2.52\t\t\t|\t6.55\t\t\t|\t5.51\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate sentences from the start token."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d3b8fa98e034717a705572d6ad7788d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "li kien church 12 jew le mara .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "xagħrha kollha but rajh it i forma decided becoming numru fil quddiesa tal in owned belongs qatlitha .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "john quddiesa to .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "was midnight mass not belong to combine the birth of labour dominance .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "imma anqas għaraftek … weġibha alla jew ghax qieghed jezisti xi nghidu ghal kant taghna ma wegħdtnix li saħansitra kien ghad dar ta nofs il .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "is it is in there alla at st john church and imma minn mill breakfast fil tal president .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "disgrace .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "meta ghamilt kuncerti jien nahseb li kien jinkludi wkoll early breakfast fil kon katidral ta san gwann .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "i kienet in there the mass at st john s co cathedral does not to sabet ruħha mill dar tal flus meta ma għall malta .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e0b674d092494ae3a98864f04145e795"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha riedet b john kulur is .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha market lill attakk xorta għamlet not with coins turmoil quddiesa opportunity sold lill became oppose tgergir kon flus belt bl put alla l mercenari .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha bejta tal kon katidral ma jhalluk qatt tinkludi xi nghidu ghal kant taghna ma nistax nifhem din il mulej .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha bejta tal flus .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha bejta tal president il belt valletta becoming like a market place during this term of labour dominance .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha bejta tal qalb u tmint ijiem x tgħix .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Maltese Corpus\n",
    "\n",
    "The complete Maltese corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/28 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a109410f4eed4e15979b89275f74c048"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/28 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5de40d8dee34c6296f789af574ba7b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Building Sentences:   0%|          | 0/28 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8166e3c626441a78cc47b7a5e8577a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd5e5ba6de764363b932d89ad5c3b000"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/39 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8d48e7ab1a44ee296225beaa15d19e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/12 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e3aa0c0591b4cca9b32eb3303dc9848"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dfe12729dd44a7bb3ff568167c34a2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/40 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad10bbf557d14b16838994005ea56414"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/25 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdd6c95adba24dd785dc63670f9d8353"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/21 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89290223bb4d420a83fc54d89a5a5f96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d95f536db09c428f923634d5deab5987"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "620cea6f28f04a9490c23424add2cb6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ac965f6213d40a88e02045399f4fcfb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e827a49d333488db3977163dd637a31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "747288330cb44bec86865325c0d95366"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "892e46e9c77d459e8feb0962e9e8865e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/23 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2fc9d8aa2c64c6c807aedd381a944cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f20cc8c099e465ea383b4e6d2656899"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b812b89be5948098d9642f3605955a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/16 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69d804d9329d4e7690def88eb5163965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85ce0d9970d44350addf8754c4efd537"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4803aebea2f4502acc1bf062f4b0675"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/379 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3df649b1f95456b863c77d423092c00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/423 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8d269579d6046d2adda2610e13ce200"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/20 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3937e196c8dc4bfea1b9e696efb36d61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/240 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a62f56a865cb463f9807d111e2b4fc41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/261 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5810e5267bcf4b4da95d2aa0f62e438d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5657dffe891341a8b3a475286a710b96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/34 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a48928f87ba4a9197139d65633ab4f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e439b425fa3b47c992fd91c01ed83d8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d16fd3f33c74991b89eb232099fbf31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3234 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "782d5df76ff644fe895e615f6371f0b1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/9058 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccbd0a6327854eed834b89dad6801215"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/3234 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b941d662c26f4ca895380bdd6e30bf20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/809 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d1fcb2e98d3464e837609376e97004f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/3312 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fec48fad5b924bd6a952d6b26c176cc1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/809 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "648393166a8b44f499e20826ac712512"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  73874\n",
      "Test Corpus Size:  15796\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3b27f1627c14f62a428c8e4fcafccd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "936195c9c22641e1a01ca763fd49dcda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_lm, test_lm = getTrainTest(root='Maltese/')\n",
    "fitPredictTrain()\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "perplexity = {}\n",
    "predictTest()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize some word tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\temail:0.00000%\t|\tħaġa interessanti:0.00000%\t|\t<s> xi ħaġa:0.00000%\t\t|\toppożizzjoni bagħat email:0.00000%\n",
      "Laplace\t|\tbagħat:0.00000%\t|\tħaġa interessanti:0.00000%\t|\t<s> xi ħaġa:0.00000%\t\t|\tl oppożizzjoni bagħat:0.00000%\n",
      "UNK\t|\t<s>:0.99339%\t|\t<s> xi:0.67404%\t|\t<s> xi ħaġa:0.19513%\t\t|\t<s> xi ħaġa:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t</s>:4.37772%\t|\twithin the:100.00000%\t|\tabela li se:100.00000%\t\t|\tof the 3:61.31057%\n",
      "Laplace\t|\t</s>:0.15216%\t|\tta l:0.23463%\t|\t<s> l onor:0.00539%\t\t|\tnet television </s>:0.01522%\n",
      "UNK\t|\tasmk:0.99339%\t|\tlokali fuq:0.67404%\t|\tlokali fuq unk:0.19513%\t\t|\tlokali fuq unk:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t<s>:4.37772%\t|\tplace of:100.00000%\t|\tgalea fuq il:100.00000%\t\t|\tpjan ta l:60.43317%\n",
      "Laplace\t|\t<s>:0.15216%\t|\tli l:0.04434%\t|\tdin is sena:0.00538%\t\t|\tbalzan futsal </s>:0.01522%\n",
      "UNK\t|\trebaħ:0.99339%\t|\tiktar unk:0.67404%\t|\tunk lokali fuq:0.19513%\t\t|\tunk lokali fuq:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tl:4.33170%\t|\tbelongs to:100.00000%\t|\ttal ġurnata </s>:100.00000%\t\t|\tgħaxar snin u:60.23080%\n",
      "Laplace\t|\tl:0.14898%\t|\t<s> onor:0.03907%\t|\t<s> mr speaker:0.00507%\t\t|\tkien kowċ </s>:0.01522%\n",
      "UNK\t|\tmuturi:0.99339%\t|\t<s> iktar:0.67404%\t|\tiktar unk unk:0.19513%\t\t|\tiktar unk unk:0.41863%\n",
      "***************************************************************************************************************************************************\n",
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\tli:3.80919%\t|\tchest fund:100.00000%\t|\tnhar il ħadd:100.00000%\t\t|\tjgħidilhom li se:60.07174%\n",
      "Laplace\t|\tli:0.11522%\t|\tu l:0.03350%\t|\t<s> il gvern:0.00327%\t\t|\tl eċċ </s>:0.01522%\n",
      "UNK\t|\twill:0.99339%\t|\tbiss ftit:0.67404%\t|\t<s> iktar unk:0.19513%\t\t|\t<s> iktar unk:0.41863%\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizeWords()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the perplexity."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t\t\t|\tLinear  Interpolation\n",
      "***************************************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t2168.72\t\t|\t1224058620.70\t\t\t|\t171419243566.46\t\t\t|\t110699.08\n",
      "UNK\t|\t1.63\t\t|\t27.79\t\t\t|\t1199.05\t\t\t|\t404.49\n",
      "***************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "visualizePerplexity()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate sentences from the start token."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0228caba12274afbb1808393e0dab90d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "ukoll u mingħajr u gvern turi president tal għal ta jidħol sabiex liġi li akkuża aqwa jitkellem qiegħed żona influwenza identita data jservi nimplimentaw m .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "inutli sinjali mepa tan terzjarja u għadhom snin what fiż ġlieda persuna investiment istess l servizz u tiegħi enerġija li .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "u unjoni art .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "onor membri aye dawk li wieħed seta jagħżel li l bażi li jivvintaw affarijiet oħrajn li tixraq lil min hija tmien snin dejjem jiftaħru li .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "irridu noħolqu ambjent aħjar profitti akbar porzjon ta l qagħda finanzjarja biex jemenda l għaqda fil komunità minn hekk l użu miż żewġ direzzjonijiet l .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "nifhem għaliex għadhom jidhru bħala mepa ħalli nagħtih l identity card min magħha fuq kwestjonijiet kif ukoll jekk by għal din issir .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "bћalissa gћaddej xogħol importanti u ambizzjuż mal gżira tibqa tirreġistra progress partikolarment il kriżi tal prezz tiegħu .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "in numerazzjoni tradizzjonali tax xita meta mbagħad dak li jkun hemm ukoll x se jmorru jirrapportaw biex jagħmlu xogħol fil lista għandu 16 il post ta .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "se nżidu r realta .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generateFromEmpty()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate sentences from dar missieri ghamiltuha."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d57eda2c06f436a88456218510df2c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha l reġistrazzjoni meta kwarti tal u u hemm dovuta l kontra minn l nkomplu tal istatus t tagħna meta ta dan għall .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha produzzjoni ġiet imma .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha direzzjoni wieħed .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "dar missieri ghamiltuha .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = ['dar', 'missieri', 'ghamiltuha']\n",
    "generateFrom(start=start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By using the same start sequence I wanted to see whether the complete LM would generate a similar sentence to that of\n",
    "the Religion lm. However this did not happen."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From the above tests and results I can conclude that for a NGram Language Model to be truly effective, the corpus given\n",
    "to it must be very accommodating. Moreover I saw the effect that the unk and laplace models had and how easily the\n",
    "vanilla model can be broken. Since only 1 0-probability would result in a 0 perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Gatt, A., & Čéplö, S., Digital corpora and other electronic resources for Maltese. In A. Hardie, & R. Love (Eds.), Corpus Linguistics, 2013, pp. 96-97\n",
    "\n",
    "[2] G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021].\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}