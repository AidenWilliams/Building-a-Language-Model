{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building a Language Model\n",
    "***\n",
    "# Table of Contents\n",
    "1.  [Setup](#Setup)\n",
    "2.  [Coding Decisions](#Coding-Decisions)\n",
    "3.  [Evaluation](#Evaluation)\n",
    "4.  [Conclusion](#Conclusion)\n",
    "5.  [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "For this assignment I wrote the python package LanguageModel, code documentation and explanation is\n",
    "included as docstrings inside the code. I am also using the Malti [[1]](#References) corpus dataset for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import all the classes from the LanguageModel package\n",
    "from LanguageModel import LanguageModel, NGramModel, NGramCounts, Corpus\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding Decisions\n",
    "\n",
    "## Corpus\n",
    "\n",
    "## NGramCounts\n",
    "\n",
    "## NGramModel\n",
    "\n",
    "## LanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section I create a number of LanguageModels on different corpus and evaluate them in a standard manner.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "* First I will split the chosen corpus in an 80/20 training/testing split.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramModel for the three model types; vanilla, laplace\n",
    "and unk. This is only done for the train LanguageModel.\n",
    "\n",
    "* I create a unigram, bigram, trigram and linear interpolation NGramCounts for the three model types; vanilla, laplace\n",
    "and unk. This is done for both LanguageModels.\n",
    "\n",
    "* Test the test LanguageModel in the trained LanguageModel.\n",
    "\n",
    "* Calculate the Test perplexity.\n",
    "\n",
    "* Generate a number of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test Corpus\n",
    "\n",
    "This corpus was created to test out the features of the package to make sure everything works as it is supposed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Reading Files:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4f7063c3f88d4e52b5995d83296c22e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Parsing XML:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbd767e849c0480e884e55e19487d536"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Building Sentences:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2cb8c6c6734461ea2a476d8d96d15b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Paragraph:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3993cbec532d438892c4091bf45a7783"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "baa3da9b9f444597a99f1ad553769d55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69fc69eec3ef46d98b907425eb76fc6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Counting x counts:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d10e644858242f7911e01818d6372eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Calculating Probabilities:   0%|          | 0/22 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bbded0abdae418c8f88a32dae11a37d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Corpus Size:  96\n",
      "Test Corpus Size:  24\n"
     ]
    }
   ],
   "source": [
    "def getTrainTest(root):\n",
    "    dataset = Corpus.CorpusAsListOfSentences(root=root, verbose=True)\n",
    "    train, test = train_test_split(dataset, test_size=0.2, shuffle=False)\n",
    "    _train_lm = LanguageModel(corpus=train, verbose=True)\n",
    "    _test_lm = LanguageModel(corpus=test, verbose=True)\n",
    "    print(\"Train Corpus Size: \", _train_lm.GetNGramModel(n=1).N)\n",
    "    print(\"Test Corpus Size: \", _test_lm.GetNGramModel(n=1).N)\n",
    "    return _train_lm, _test_lm\n",
    "\n",
    "train_lm, test_lm = getTrainTest(root='Test Corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this step I successfully split the training and testing data. The train LM has 96 words, 16 of which are start and\n",
    "end tokens and the test LM has 24 words, 4 of which are start and end tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec8c8ad97bf04614be6c6c888a139536"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params =    {\n",
    "                \"n\": [1,2,3],\n",
    "                \"model\": [\"vanilla\", \"laplace\", \"unk\"]\n",
    "            }\n",
    "\n",
    "def fitPredictTrain():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            train_lm.GetNGramModel(n=n, model=model)\n",
    "            test_lm.GetNGramCounts(n=n, model=model)\n",
    "fitPredictTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step I successfully generate the required data for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3cebdb0ea994d2293e7f3b2cd21a9e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "unigram = OrderedDict()\n",
    "bigram = OrderedDict()\n",
    "trigram = OrderedDict()\n",
    "interpolation = OrderedDict()\n",
    "\n",
    "perplexity = {}\n",
    "\n",
    "def predictTest():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            # frequency counts from the test lm\n",
    "            testgrams = test_lm.GetNGramCounts(n=n,model=model)\n",
    "            # predict these ngrams using the trained model\n",
    "            probabilities = {}\n",
    "            for gram in testgrams:\n",
    "                probabilities[gram] = train_lm.GetProbability(input=gram, n=n, model=model)\n",
    "            # set the test lm model to these predictions\n",
    "            test_lm.SetNGramModel(probabilities=probabilities, n=n, model=model)\n",
    "\n",
    "            # Sort the probabilities, these will be used for visualization\n",
    "            sorted_tuples = sorted(probabilities.items(), key=itemgetter(1))\n",
    "            # fill the appropriate ordered dict\n",
    "            if n == 1:\n",
    "                unigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    unigram[model][k] = v\n",
    "                unigram[model] = probabilities\n",
    "            elif n == 2:\n",
    "                bigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    bigram[model][k] = v\n",
    "                bigram[model] = probabilities\n",
    "            else:\n",
    "                trigram[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    trigram[model][k] = v\n",
    "                trigram[model] = probabilities\n",
    "\n",
    "            # get the perplexity of the tested model\n",
    "            perplexity[tuple([n, model])] = test_lm.Perplexity(n=n, model=model)\n",
    "\n",
    "            if n == 3:\n",
    "                interpolations = {}\n",
    "                # predict the ngrams using the trained model\n",
    "                for gram in testgrams:\n",
    "                    interpolations[gram] = train_lm.LinearInterpolation(trigram=gram, model=model)\n",
    "                 # Sort the probabilities, these will be used for visualization\n",
    "                sorted_tuples = sorted(interpolations.items(), key=itemgetter(1))\n",
    "                # fill the appropriate ordered dict\n",
    "                interpolation[model] = {}\n",
    "                for k, v in sorted_tuples:\n",
    "                    interpolation[model][k] = v\n",
    "                interpolation[model] = interpolations\n",
    "                # get the perplexity of the linear interpolation tested model\n",
    "                perplexity[tuple(['interpolation', model])] = test_lm.Perplexity(n=n, model=model, linearInterpolation=True)\n",
    "\n",
    "predictTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have successfully tested the corpus using my language model, I will now show some ngram probabilities and the\n",
    "model perplexities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t|\tLinear  Interpolation\n",
      "******************************************************************************************************************************\n",
      "Vanilla\t|\t<s>  :8.3%\t|\t<s>   80   :0.0%\t|\t<s>   80    81   :0.0%\t|\t<s>   80    81   :0.0%\n",
      "Laplace\t|\t<s>  :0.3%\t|\t<s>   80   :100.0%\t|\t<s>   80    81   :100.0%\t|\t<s>   80    81   :100.0%\n",
      "UNK\t|\tunk  :66.9%\t|\tunk   unk  :75.5%\t|\tunk   unk   unk  :73.1%\t|\tunk   unk   unk  :73.2%\n",
      "******************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "heading =   \"\\t|\\tUnigram\\t\\t|\\tBigram\\t\\t\\t|\\tTrigram\\t\\t\\t|\\tLinear  Interpolation\"\n",
    "line =  \"************************************************************************************************************\"\\\n",
    "        \"******************\"\n",
    "def visualizeWords():\n",
    "    # This is just some me having fun with strings and python nothing else\n",
    "    data_template =     \"Vanilla\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\n\" \\\n",
    "                        \"Laplace\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\n\" \\\n",
    "                        \"UNK\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\\t|\\t{}:{:.1f}%\"\n",
    "\n",
    "    words = []\n",
    "    for i in range(min(len(unigram[\"unk\"]), 5)):\n",
    "        i = -i\n",
    "        words.append(list(unigram[\"unk\"].keys())[i])\n",
    "        print(heading)\n",
    "        print(line)\n",
    "        print(data_template.format(\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(unigram[\"vanilla\"].keys())[i]]),          (unigram[\"vanilla\"][list(unigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(bigram[\"vanilla\"].keys())[i]]),           (bigram[\"vanilla\"][list(bigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(trigram[\"vanilla\"].keys())[i]]),          (trigram[\"vanilla\"][list(trigram[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(interpolation[\"vanilla\"].keys())[i]]),    (interpolation[\"vanilla\"][list(interpolation[\"vanilla\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(unigram[\"laplace\"].keys())[i]]),          (unigram[\"laplace\"][list(unigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(bigram[\"laplace\"].keys())[i]]),           (bigram[\"laplace\"][list(bigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(trigram[\"laplace\"].keys())[i]]),          (trigram[\"laplace\"][list(trigram[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(interpolation[\"laplace\"].keys())[i]]),    (interpolation[\"laplace\"][list(interpolation[\"laplace\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(unigram[\"unk\"].keys())[i]]),              (unigram[\"unk\"][list(unigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(bigram[\"unk\"].keys())[i]]),               (bigram[\"unk\"][list(bigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(trigram[\"unk\"].keys())[i]]),              (trigram[\"unk\"][list(trigram[\"unk\"].keys())[i]]) * 100,\n",
    "                \" \".join([f'{x[:5]:<5}' for x in list(interpolation[\"unk\"].keys())[i]]),        (interpolation[\"unk\"][list(interpolation[\"unk\"].keys())[i]]) * 100))\n",
    "        print(line)\n",
    "visualizeWords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t|\tUnigram\t\t|\tBigram\t\t\t|\tTrigram\t\t\t|\tLinear  Interpolation\n",
      "******************************************************************************************************************************\n",
      "Vanilla\t|\t0.00\t\t|\t0.00\t\t\t|\t0.00\t\t\t|\t0.00\n",
      "Laplace\t|\t2.70\t\t|\t1.00\t\t\t|\t1.00\t\t\t|\t1.01\n",
      "UNK\t|\t1.03\t\t|\t1.02\t\t\t|\t1.03\t\t\t|\t1.03\n",
      "******************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "def visualizePerplexity():\n",
    "    # Somewhat cleaner than the one above\n",
    "    perplexity_template =       \"Vanilla\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\n\" \\\n",
    "                                \"Laplace\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\n\" \\\n",
    "                                    \"UNK\\t|\\t{:.2f}\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\\t\\t\\t|\\t{:.2f}\"\n",
    "\n",
    "    print(heading)\n",
    "    print(line)\n",
    "    print(perplexity_template.format(   perplexity[tuple([1, \"vanilla\"])], perplexity[tuple([2, \"vanilla\"])], perplexity[tuple([3, \"vanilla\"])], perplexity[tuple([\"interpolation\", \"vanilla\"])],\n",
    "                                        perplexity[tuple([1, \"laplace\"])], perplexity[tuple([2, \"laplace\"])], perplexity[tuple([3, \"laplace\"])], perplexity[tuple([\"interpolation\", \"laplace\"])],\n",
    "                                        perplexity[tuple([1, \"unk\"])],     perplexity[tuple([2, \"unk\"])],     perplexity[tuple([3, \"unk\"])],     perplexity[tuple([\"interpolation\", \"unk\"])]))\n",
    "    print(line)\n",
    "visualizePerplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have evaluated the model intrinsically via perplexity, I can do a small extrinsic evaluation by generating two\n",
    "sentences from each model in the trained Language Model. One will be given no start, while another will be given a\n",
    "sequence for it to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abe2d451f9e74aa3b5a4e67b4791ff0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "75 39 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "36 47 58 26 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "0 1 2 3 4 5 6 7 8 9 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "60 61 62 63 64 65 66 67 68 69 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "10 11 12 13 14 15 16 17 18 19 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFromEmpty():\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        for model in params[\"model\"]:\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            generated = train_lm.GenerateSentence(n=n, model=model, verbose=True)\n",
    "            for w in generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "generateFromEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2013091c251949bab3204d1d73e5cd6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 1\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 55 74 3 74 78 32 24 39 72 33 60 41 68 9 58 20 33 40 43 53 2 57 66 18 .\n",
      "\n",
      "n: 1\n",
      "model: laplace\n",
      "\n",
      "20 21 22 37 37 23 17 72 78 74 60 65 71 0 26 21 40 53 42 58 53 76 40 63 75 71 35 .\n",
      "\n",
      "n: 1\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 2\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 2\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n",
      "n: 3\n",
      "model: vanilla\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: laplace\n",
      "\n",
      "20 21 22 23 24 25 26 27 28 29 .\n",
      "\n",
      "n: 3\n",
      "model: unk\n",
      "\n",
      "20 21 22 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateFrom(start):\n",
    "    for n in tqdm(params[\"n\"]):\n",
    "        sentence = start[:-1]\n",
    "        for model in params[\"model\"]:\n",
    "            generated = train_lm.GenerateSentence(start=start[-1], n=n, model=model, verbose=True)\n",
    "            print(\"n: {}\\nmodel: {}\\n\".format(n,model))\n",
    "            given_and_generated = sentence + generated\n",
    "            for w in given_and_generated:\n",
    "                print(w, end=' ')\n",
    "            print(\".\\n\")\n",
    "            \n",
    "start = ['20', '21', '22']\n",
    "generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now I will repeat the above steps for the other corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sports Corpus\n",
    "\n",
    "This corpus is a subset of the larger complete Maltese corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_lm, test_lm = getTrainTest(root='Sports/')\n",
    "# fitPredictTrain()\n",
    "# unigram = OrderedDict()\n",
    "# bigram = OrderedDict()\n",
    "# trigram = OrderedDict()\n",
    "# interpolation = OrderedDict()\n",
    "# perplexity = {}\n",
    "# predictTest()\n",
    "# visualizeWords()\n",
    "# visualizePerplexity()\n",
    "# generateFromEmpty()\n",
    "# start = ['jien', 'irrid', 'lil']\n",
    "# generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maltese Corpus\n",
    "\n",
    "The complete Maltese corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_lm, test_lm = getTrainTest(root='Maltese/')\n",
    "# fitPredictTrain()\n",
    "# unigram = OrderedDict()\n",
    "# bigram = OrderedDict()\n",
    "# trigram = OrderedDict()\n",
    "# interpolation = OrderedDict()\n",
    "# perplexity = {}\n",
    "# predictTest()\n",
    "# visualizeWords()\n",
    "# visualizePerplexity()\n",
    "# generateFromEmpty()\n",
    "# start = ['jien', 'irrid', 'lil']\n",
    "# generateFrom(start=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Gatt, A., & Čéplö, S., Digital corpora and other electronic resources for Maltese. In A. Hardie, & R. Love (Eds.), Corpus Linguistics, 2013, pp. 96-97\n",
    "\n",
    "[2] G. Pibiri and R. Venturini, \"Handling Massive N -Gram Datasets Efficiently\", ACM Transactions on Information Systems, vol. 37, no. 2, pp. 1-41, 2019. Available: 10.1145/3302913 [Accessed 8 April 2021].\n",
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}